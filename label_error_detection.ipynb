{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torchvision.ops import nms, box_iou\n",
    "import torch\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import ast\n",
    "#from plot import set_plot_layout\n",
    "\n",
    "from tools_label_error_detection import construct_original_gt, construct_dont_care_regions, draw_boxes\n",
    "\n",
    "#set_plot_layout(path_to_latex = '/home/jklees/texlive/bin/x86_64-linux');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "\n",
    "np.random.seed(0) # this just influences the random selection of images for display\n",
    "\n",
    "\n",
    "iou_threshold_misfitting_box = 0.5  # IoU threshold for considering a misfitting box as a label error (Comparison of original and validated GT boxes)\n",
    "iou_thresh_dont_care = 0.5 # IoU threshold for considering the intersection with dont care regions i.e. determining whether annotations are actual label errors or were simply not annotated on purpose.\n",
    "\n",
    "# data parameters\n",
    "validated_gt_prob_threshold = 0.0  # Probability threshold for ground truth class pedestrian (0.5 and 0.8 available)\n",
    "min_bbox_height = 25 # minimal height of a bounding box to be considered as a pedestrian (according to KITTI Benchmark moderate and hard version). Applies to both original and validated GT boxes.\n",
    "filter_small_boxes = False # Whether to filter out boxes that are too small\n",
    "filter_dont_care = False\n",
    "\n",
    "n_images_to_display = 0\n",
    "path_to_kitti = '/home/datasets_archive/KITTI/training/'\n",
    "path_to_split = \"data/train_val_split.json\"\n",
    "# path_to_kitti_val_csv_files = \"data/KITTI/gt/gt_val_csv_files/csv/\"\n",
    "# path_to_kitti_dont_care_regions = \"data/KITTI/val_dont_care.json\"\n",
    "path_to_validated_gt = \"data/validated_gt.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Label errors in the original dataset \n",
    "\n",
    "Here, we evaluate the quality of the orginal dataset by comparing it to the validated ground truth we constructed. \n",
    "A label error is here defined as an overlooked pedestrian or a box for a pedestrian that does not fit well (IoU < 0.5 compared to our validated GT)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_data_for_conditions(df):\n",
    "    if filter_dont_care:\n",
    "        df = df[df[\"iou_with_dont_care\"] < iou_thresh_dont_care]\n",
    "    if filter_small_boxes:\n",
    "        df = df[df[\"height\"] >= min_bbox_height]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>xmin</th>\n",
       "      <th>ymin</th>\n",
       "      <th>xmax</th>\n",
       "      <th>ymax</th>\n",
       "      <th>class_name</th>\n",
       "      <th>class</th>\n",
       "      <th>height</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>003206.png</td>\n",
       "      <td>584.97</td>\n",
       "      <td>172.95</td>\n",
       "      <td>590.64</td>\n",
       "      <td>194.60</td>\n",
       "      <td>Pedestrian</td>\n",
       "      <td>2</td>\n",
       "      <td>21.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>003206.png</td>\n",
       "      <td>590.29</td>\n",
       "      <td>173.84</td>\n",
       "      <td>596.29</td>\n",
       "      <td>194.88</td>\n",
       "      <td>Pedestrian</td>\n",
       "      <td>2</td>\n",
       "      <td>21.04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     filename    xmin    ymin    xmax    ymax  class_name  class  height\n",
       "0  003206.png  584.97  172.95  590.64  194.60  Pedestrian      2   21.65\n",
       "1  003206.png  590.29  173.84  596.29  194.88  Pedestrian      2   21.04"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(path_to_split, \"r\") as f:\n",
    "    split = json.load(f)  # `data` is now a Python dict\n",
    "\n",
    "# Construct the original annotations as a DataFrame\n",
    "original_gt = construct_original_gt(path_to_kitti, split, filter_small_boxes, min_bbox_height)\n",
    "\n",
    "# construct the dont care regions from the original dataset as a DataFrame\n",
    "dont_care_regions = construct_dont_care_regions(path_to_kitti, split)\n",
    "\n",
    "original_gt.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>xmin</th>\n",
       "      <th>ymin</th>\n",
       "      <th>xmax</th>\n",
       "      <th>ymax</th>\n",
       "      <th>class_name</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>007359.png</td>\n",
       "      <td>400.29</td>\n",
       "      <td>158.29</td>\n",
       "      <td>492.82</td>\n",
       "      <td>197.37</td>\n",
       "      <td>DontCare</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>007359.png</td>\n",
       "      <td>875.12</td>\n",
       "      <td>155.21</td>\n",
       "      <td>911.12</td>\n",
       "      <td>195.32</td>\n",
       "      <td>DontCare</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     filename    xmin    ymin    xmax    ymax class_name  class\n",
       "0  007359.png  400.29  158.29  492.82  197.37   DontCare     -1\n",
       "1  007359.png  875.12  155.21  911.12  195.32   DontCare     -1"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dont_care_regions.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dont_care_regions.to_csv(\"data/original_gt_dont_care_regions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>media_object_id</th>\n",
       "      <th>media_id</th>\n",
       "      <th>filename</th>\n",
       "      <th>bbox</th>\n",
       "      <th>score</th>\n",
       "      <th>xmin</th>\n",
       "      <th>ymin</th>\n",
       "      <th>xmax</th>\n",
       "      <th>ymax</th>\n",
       "      <th>height</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>02bbfce0-6cd9-45ef-b21a-082aab0ff1dd</td>\n",
       "      <td>d9051eda-343d-4e4c-839a-3b8f8c24bcd4</td>\n",
       "      <td>002112.png</td>\n",
       "      <td>[554.18, 189.065, 10.240000000000009, 25.43000...</td>\n",
       "      <td>0.197628</td>\n",
       "      <td>549.06</td>\n",
       "      <td>176.35</td>\n",
       "      <td>559.30</td>\n",
       "      <td>201.78</td>\n",
       "      <td>25.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>718cc290-c20a-4e54-980f-eabeabf5dec7</td>\n",
       "      <td>04156827-cee7-4ad0-8329-a9464988ffc8</td>\n",
       "      <td>007256.png</td>\n",
       "      <td>[556.235, 190.315, 7.690000000000055, 15.37000...</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>552.39</td>\n",
       "      <td>182.63</td>\n",
       "      <td>560.08</td>\n",
       "      <td>198.00</td>\n",
       "      <td>15.37</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        media_object_id                              media_id  \\\n",
       "0  02bbfce0-6cd9-45ef-b21a-082aab0ff1dd  d9051eda-343d-4e4c-839a-3b8f8c24bcd4   \n",
       "1  718cc290-c20a-4e54-980f-eabeabf5dec7  04156827-cee7-4ad0-8329-a9464988ffc8   \n",
       "\n",
       "     filename                                               bbox     score  \\\n",
       "0  002112.png  [554.18, 189.065, 10.240000000000009, 25.43000...  0.197628   \n",
       "1  007256.png  [556.235, 190.315, 7.690000000000055, 15.37000...  0.083333   \n",
       "\n",
       "     xmin    ymin    xmax    ymax  height  \n",
       "0  549.06  176.35  559.30  201.78   25.43  \n",
       "1  552.39  182.63  560.08  198.00   15.37  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the validated ground truth data\n",
    "validated_gt = pd.read_csv(path_to_validated_gt)\n",
    "validated_gt = validated_gt[validated_gt[\"score\"] >= validated_gt_prob_threshold] # filter by the probability threshold\n",
    "validated_gt.reset_index(drop=True, inplace=True)\n",
    "# Ensure the 'filename' column is in the correct format\n",
    "validated_gt[\"filename\"] = validated_gt['filename'].astype(str).str.zfill(6) + '.png'\n",
    "# Convert box format from x_mean, y_mean, width, height to xmin, ymin, xmax, ymax\n",
    "validated_gt[\"xmin\"] = validated_gt[\"bbox\"].apply(lambda x: ast.literal_eval(x)[0] - ast.literal_eval(x)[2] / 2)\n",
    "validated_gt[\"ymin\"] = validated_gt[\"bbox\"].apply(lambda x: ast.literal_eval(x)[1] - ast.literal_eval(x)[3] / 2)\n",
    "validated_gt[\"xmax\"] = validated_gt[\"bbox\"].apply(lambda x: ast.literal_eval(x)[0] + ast.literal_eval(x)[2] / 2 )\n",
    "validated_gt[\"ymax\"] = validated_gt[\"bbox\"].apply(lambda x: ast.literal_eval(x)[1] + ast.literal_eval(x)[3] / 2)\n",
    "\n",
    "validated_gt[\"height\"] = validated_gt[\"ymax\"] - validated_gt[\"ymin\"]\n",
    "if filter_small_boxes:\n",
    "    print(f\"Filtering out {len(validated_gt[validated_gt['height'] < min_bbox_height])} boxes with height < {min_bbox_height} pixels\")\n",
    "    validated_gt = validated_gt[validated_gt[\"height\"] >= min_bbox_height]  # Filter out boxes that are too small\n",
    "    validated_gt.reset_index(drop=True, inplace=True)  # Reset the index\n",
    "\n",
    "validated_gt.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pedestrian boxes in the original dataset: 896\n",
      "Number of pedestrian boxes in the validated dataset: 3078\n",
      "Validated ground truth contains 2182 more boxes than the orig. GT.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of pedestrian boxes in the original dataset: {len(original_gt)}\")\n",
    "print(f\"Number of pedestrian boxes in the validated dataset: {len(validated_gt)}\")\n",
    "print(f\"Validated ground truth contains {len(validated_gt) - len(original_gt)} more boxes than the orig. GT.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare annotations of the original ground truth with the validated ground truth also plotting the dont care regions\n",
    "image_ids = np.random.choice(validated_gt['filename'].unique(), size = n_images_to_display, replace=False) # Get unique image filenames\n",
    "\n",
    "for image_id in image_ids: \n",
    "    # Read the predicted boxes from the DataFrame\n",
    "    box_indices = validated_gt[validated_gt['filename'] == image_id].index\n",
    "    boxes = [validated_gt.iloc[i][['xmin', 'ymin', 'xmax', 'ymax', 'score']].values.reshape(1, -1) for i in box_indices]\n",
    "    boxes = np.vstack(boxes)  # Stack all predicted boxes for this image\n",
    "\n",
    "    dont_care_box_indices = dont_care_regions[dont_care_regions['filename'] == image_id].index\n",
    "    dont_care_boxes = [dont_care_regions.iloc[i][['xmin', 'ymin', 'xmax', 'ymax']].values.reshape(1, -1) for i in dont_care_box_indices]\n",
    "    dont_care_boxes = np.vstack(dont_care_boxes) if len(dont_care_boxes) > 0 else None # Stack all predicted boxes for this image\n",
    "\n",
    "    image_id = image_id.split('.')[0]  # Remove file extension if present\n",
    "\n",
    "    draw_boxes(f'{path_to_kitti}image_2/{image_id}.png', f'{path_to_kitti}label_2/{image_id}.txt', pred_boxes=boxes, add_pred_boxes=dont_care_boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def determine_label_errors_in_original_gt(original_gt, validated_gt, iou_threshold_misfitting_box):\n",
    "#     \"\"\"\n",
    "#     Determine label errors in the original ground truth through comparison with the validated ground truth.\n",
    "    \n",
    "#     Parameters:\n",
    "#     - original_gt: DataFrame containing the original ground truth boxes.\n",
    "#     - validated_gt: DataFrame containing the validated ground truth boxes.\n",
    "#     - iou_threshold_misfitting_box: IoU threshold for considering a misfitting box as a label error.\n",
    "    \n",
    "#     Returns:\n",
    "#     - updated validated_gt DataFrame with evaluation of all boxes with respect to whether they were not part of the original GT.\n",
    "#       (For each box, we not whether this box marks an actual Label Error of the original GT and store the Label Error Type).\n",
    "#     \"\"\"\n",
    "\n",
    "#     bool_label_errors = [False] * len(validated_gt)\n",
    "#     label_error_type = [\"no\"]*len(validated_gt)  # To store the type of label error (no, overlooked pedestrian or misfitting box)\n",
    "#     label_errors = 0 # not returned because it can be easily determined from the DataFrame\n",
    "#     for i, row in validated_gt.iterrows():\n",
    "\n",
    "#         original_boxes = original_gt[(original_gt['filename'] == row['filename']) ]\n",
    "        \n",
    "#         if original_boxes.empty: # if there are no boxes with the same filename in the original ground truth we have a label error (pedestrian was overlooked in that image)\n",
    "#             label_errors += 1\n",
    "#             bool_label_errors[i] = True\n",
    "#             label_error_type[i] = \"overlooked pedestrian\"\n",
    "#         else:\n",
    "#             # If there are boxes annotated with pedestrian in the GT for this image, we check if any of them matches the validated ground truth box (IoU >= 0.5)\n",
    "#             ious = []\n",
    "#             for _, original_box in original_boxes.iterrows():\n",
    "#                 # Calculate IoU between the validated ground truth box and the original boxes on this image\n",
    "#                 # If the IoU is >= 0.5 for one of the boxes, we consider it a match and do not count it as a label error\n",
    "#                 original_box = original_box.to_frame().T # pd.Series to DataFrame\n",
    "#                 # Calculate IoU using torchvision.ops.box_iou\n",
    "\n",
    "#                 validated_box = torch.tensor(np.array(row[['xmin', 'ymin', 'xmax', 'ymax']].values, dtype = float), dtype=torch.float32).unsqueeze(0)\n",
    "#                 original_box = torch.tensor(np.array([original_box.xmin, original_box.ymin, original_box.xmax, original_box.ymax], dtype=float).flatten(), dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "#                 iou = box_iou(validated_box, original_box)[0, 0].item()\n",
    "#                 ious.append(iou)\n",
    "\n",
    "#             if max(ious) < iou_threshold_misfitting_box:\n",
    "#                 label_errors += 1\n",
    "#                 bool_label_errors[i] = True\n",
    "#                 if max(ious) == 0:\n",
    "#                     label_error_type[i] = \"overlooked pedestrian\"\n",
    "#                 else:\n",
    "#                     # If the IoU is below the threshold but not zero, we consider it a misfitting box\n",
    "#                     label_error_type[i] = \"misfitting box\"\n",
    "\n",
    "#     validated_gt['label_error'] = bool_label_errors\n",
    "#     validated_gt['label_error_type'] = label_error_type\n",
    "\n",
    "#     return validated_gt\n",
    "\n",
    "def determine_label_errors_in_original_gt(original_gt, validated_gt, iou_threshold_misfitting_box):\n",
    "    bool_label_errors = [False] * len(validated_gt)\n",
    "    label_error_type = [\"no\"] * len(validated_gt)\n",
    "    label_errors = 0\n",
    "\n",
    "    matched_original_indices = set()\n",
    "\n",
    "    for i, row in validated_gt.iterrows():\n",
    "        filename = row['filename']\n",
    "        validated_box = torch.tensor(row[['xmin', 'ymin', 'xmax', 'ymax']].values.astype(float), dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "        original_boxes = original_gt[original_gt['filename'] == filename]\n",
    "\n",
    "        best_iou = 0\n",
    "        best_match_idx = None\n",
    "\n",
    "        for j, original_box in original_boxes.iterrows():\n",
    "            if j in matched_original_indices:\n",
    "                continue  # already matched\n",
    "\n",
    "            original_box_tensor = torch.tensor([\n",
    "                float(original_box['xmin']), float(original_box['ymin']),\n",
    "                float(original_box['xmax']), float(original_box['ymax'])\n",
    "            ], dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "            iou = box_iou(validated_box, original_box_tensor)[0, 0].item()\n",
    "            if iou > best_iou:\n",
    "                best_iou = iou\n",
    "                best_match_idx = j\n",
    "\n",
    "        if best_iou >= iou_threshold_misfitting_box:\n",
    "            matched_original_indices.add(best_match_idx)  # mark match\n",
    "        else:\n",
    "            label_errors += 1\n",
    "            bool_label_errors[i] = True\n",
    "            if best_iou == 0:\n",
    "                label_error_type[i] = \"overlooked pedestrian\"\n",
    "            else:\n",
    "                label_error_type[i] = \"misfitting box\"\n",
    "\n",
    "    validated_gt['label_error'] = bool_label_errors\n",
    "    validated_gt['label_error_type'] = label_error_type\n",
    "    return validated_gt\n",
    "\n",
    "def get_ious_with_orig_and_val_gt(df):\n",
    "    \"\"\"\n",
    "    This function calculates the IoU of each predicted box in the DataFrame with the original ground truth (GT) and the validated GT.\n",
    "    It adds two new columns to the DataFrame: 'iou_with_original_gt' and 'iou_with_val_gt'.\n",
    "    \"\"\"\n",
    "    # get IoU with original GT for each predicted box to select label error proposals (IoU < 0.5)\n",
    "    orig_ious = []\n",
    "    for i, row in df.iterrows():\n",
    "        # Get the corresponding predicted box\n",
    "        pred_box = torch.tensor([row['xmin'], row['ymin'], row['xmax'], row['ymax']], dtype=torch.float32).unsqueeze(0)\n",
    "        \n",
    "        # Get the ground truth boxes for the same image\n",
    "        original_gt_boxes = original_gt[original_gt['filename'] == row['filename']][['xmin', 'ymin', 'xmax', 'ymax']].values\n",
    "        \n",
    "        if len(original_gt_boxes) > 0:\n",
    "            original_gt_boxes_tensor = torch.tensor(original_gt_boxes, dtype=torch.float32)\n",
    "            iou = box_iou(pred_box, original_gt_boxes_tensor).squeeze().numpy()\n",
    "            orig_ious.append(iou.max())\n",
    "        else:\n",
    "            orig_ious.append(0)\n",
    "\n",
    "    df['iou_with_original_gt'] = orig_ious\n",
    "\n",
    "    # get IoU with dont care regions of the original GT for each predicted box to determine whether the object has been flagged as not of interest.\n",
    "    dc_ious = []\n",
    "    for i, row in df.iterrows():\n",
    "        # Get the corresponding predicted box\n",
    "        pred_box = torch.tensor([row['xmin'], row['ymin'], row['xmax'], row['ymax']], dtype=torch.float32).unsqueeze(0)\n",
    "        \n",
    "        # Get the ground truth boxes for the same image\n",
    "        dc_boxes = dont_care_regions[dont_care_regions['filename'] == row['filename']][['xmin', 'ymin', 'xmax', 'ymax']].values\n",
    "        \n",
    "        if len(dc_boxes) > 0:\n",
    "            dc_boxes_tensor = torch.tensor(dc_boxes, dtype=torch.float32)\n",
    "            iou = box_iou(pred_box, dc_boxes_tensor).squeeze().numpy()\n",
    "            dc_ious.append(iou.max())\n",
    "        else:\n",
    "            dc_ious.append(0)\n",
    "\n",
    "    df['iou_with_dont_care'] = dc_ious\n",
    "\n",
    "    # get IoU with validated GT for each predicted box in the overlooked DataFrame to determine label errors of this type\n",
    "    val_ious = []\n",
    "    for i, row in df.iterrows():\n",
    "        # Get the corresponding predicted box\n",
    "        pred_box = torch.tensor([row['xmin'], row['ymin'], row['xmax'], row['ymax']], dtype=torch.float32).unsqueeze(0)\n",
    "        \n",
    "        # Get the validated ground truth boxes for the same image\n",
    "        val_gt_boxes = validated_gt[validated_gt['filename'] == row['filename']][['xmin', 'ymin', 'xmax', 'ymax']].values\n",
    "        \n",
    "        if len(val_gt_boxes) > 0:\n",
    "            val_gt_boxes_tensor = torch.tensor(val_gt_boxes, dtype=torch.float32)\n",
    "            iou = box_iou(pred_box, val_gt_boxes_tensor).squeeze().numpy()\n",
    "            val_ious.append(iou.max())\n",
    "        else:\n",
    "            val_ious.append(0)\n",
    "\n",
    "    df['iou_with_val_gt'] = val_ious\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of label errors (overlooked pedestrians or misfitting boxes) in the original dataset: 2353\n",
      "Overlooked pedestrians (orig. IoU = 0): 2094\n",
      "Misfitting boxes (0 < orig. IoU < thresh = 0.5): 259\n"
     ]
    }
   ],
   "source": [
    "validated_gt = determine_label_errors_in_original_gt(original_gt, validated_gt, iou_threshold_misfitting_box)\n",
    "print(f\"Number of label errors (overlooked pedestrians or misfitting boxes) in the original dataset: {len(validated_gt[validated_gt['label_error'] == True])}\")\n",
    "print(f\"Overlooked pedestrians (orig. IoU = 0): {len(validated_gt[validated_gt['label_error_type'] == 'overlooked pedestrian'])}\")\n",
    "print(f\"Misfitting boxes (0 < orig. IoU < thresh = {iou_threshold_misfitting_box}): {len(validated_gt[validated_gt['label_error_type'] == 'misfitting box'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the label errors for review purposes\n",
    "# Display a few images with label errors distinguishing between overlooked pedestrians and misfitting boxes\n",
    "\n",
    "# Green: orig. GT boxes, Red: validated GT boxes with label errors (overlooked pedestrians), Blue: validated GT boxes with label errors (misfitting boxes)\n",
    "\n",
    "n = n_images_to_display #10\n",
    "\n",
    "image_ids = np.random.choice(validated_gt[validated_gt['label_error'] == True]['filename'].unique(), size = n, replace=False) # Get unique image filenames with label errors\n",
    "for image_id in image_ids: \n",
    "    # Read the predicted boxes from the DataFrame\n",
    "    box_indices = validated_gt[(validated_gt['filename'] == image_id) & (validated_gt[\"label_error_type\"] == \"overlooked pedestrian\")].index\n",
    "    overlooked_boxes = [validated_gt.iloc[i][['xmin', 'ymin', 'xmax', 'ymax', 'score']].values.reshape(1, -1) for i in box_indices]\n",
    "    overlooked_boxes = None if len(overlooked_boxes) == 0 else np.vstack(overlooked_boxes) # Stack all predicted boxes for this image\n",
    "\n",
    "    box_indices = validated_gt[(validated_gt['filename'] == image_id) & (validated_gt[\"label_error_type\"] == \"misfitting box\")].index\n",
    "    misfitting_boxes = [validated_gt.iloc[i][['xmin', 'ymin', 'xmax', 'ymax', 'score']].values.reshape(1, -1) for i in box_indices]\n",
    "    misfitting_boxes = None if len(misfitting_boxes) == 0 else np.vstack(misfitting_boxes) # Stack all predicted boxes for this image\n",
    "\n",
    "    image_id = image_id.split('.')[0]  # Remove file extension if present\n",
    "    draw_boxes(f'{path_to_kitti}image_2/{image_id}.png', f'{path_to_kitti}label_2/{image_id}.txt', pred_boxes=overlooked_boxes, add_pred_boxes=misfitting_boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of actual label errors (overlooked pedestrians or misfitting boxes) outside of dont care regions: 2217\n",
      "Overlooked pedestrians (orig. IoU = 0): 1962\n",
      "Misfitting boxes (0 < orig. IoU < thresh = 0.5): 255\n"
     ]
    }
   ],
   "source": [
    "# Account for the dont care regions here to determine the actual label errors taking into account that objects within these regions are not annotated on purpose\n",
    "# For our study however, we consider the entire images and do not account for these regions\n",
    "\n",
    "validated_gt = get_ious_with_orig_and_val_gt(validated_gt)\n",
    "original_gt = get_ious_with_orig_and_val_gt(original_gt)\n",
    "\n",
    "actual_overlooked_pedestrians = len(validated_gt[(validated_gt[\"label_error_type\"] == \"overlooked pedestrian\") & (validated_gt[\"iou_with_dont_care\"] < iou_thresh_dont_care)])\n",
    "actual_misfitting_annotations = len(validated_gt[(validated_gt[\"label_error_type\"] == \"misfitting box\") & (validated_gt[\"iou_with_dont_care\"] < iou_thresh_dont_care)])\n",
    "print(f\"Number of actual label errors (overlooked pedestrians or misfitting boxes) outside of dont care regions: {actual_overlooked_pedestrians + actual_misfitting_annotations}\")\n",
    "print(f\"Overlooked pedestrians (orig. IoU = 0): {actual_overlooked_pedestrians}\")\n",
    "print(f\"Misfitting boxes (0 < orig. IoU < thresh = {iou_threshold_misfitting_box}): {actual_misfitting_annotations}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plot annotations that are in the original gt but not in the val. GT\n",
    "\n",
    "# These examples exist due to the fact that we set a threshold for the probability of a box actually containing a pedestrian\n",
    "\n",
    "# original_gt = get_ious_with_orig_and_val_gt(original_gt)\n",
    "\n",
    "# image_ids = original_gt['filename'][original_gt[\"iou_with_val_gt\"] == 0]\n",
    "# for image_id in image_ids: \n",
    "#     # Read the predicted boxes from the DataFrame\n",
    "#     box_indices = validated_gt[validated_gt['filename'] == image_id].index\n",
    "#     boxes = [validated_gt.iloc[i][['xmin', 'ymin', 'xmax', 'ymax', 'score']].values.reshape(1, -1) for i in box_indices]\n",
    "#     if boxes:\n",
    "#         boxes = np.vstack(boxes)  # Stack all predicted boxes for this image\n",
    "#     image_id = image_id.split('.')[0]  # Remove file extension if present\n",
    "#     draw_boxes(f'{path_to_kitti}image_2/{image_id}.png', f'{path_to_kitti}label_2/{image_id}.txt', pred_boxes=boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validated_gt = validated_gt.drop(columns=[\"iou_with_val_gt\"])\n",
    "# validated_gt.to_csv(\"data/validated_gt_new.csv\", index=False)\n",
    "\n",
    "# original_gt = original_gt.drop(columns=[\"iou_with_original_gt\", \"iou_with_val_gt\"])\n",
    "# original_gt.to_csv(\"data/original_gt.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tmp print results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overlooked pedestrians (orig. IoU = 0): 2094\n",
      "Misfitting boxes (0 < orig. IoU < thresh = 0.5): 259\n",
      "OUTSIDE DONT CARE\n",
      "Overlooked pedestrians (orig. IoU = 0): 1962\n",
      "Misfitting boxes (0 < orig. IoU < thresh = 0.5): 255\n"
     ]
    }
   ],
   "source": [
    "print(f\"Overlooked pedestrians (orig. IoU = 0): {len(validated_gt[validated_gt['label_error_type'] == 'overlooked pedestrian'])}\")\n",
    "print(f\"Misfitting boxes (0 < orig. IoU < thresh = {iou_threshold_misfitting_box}): {len(validated_gt[validated_gt['label_error_type'] == 'misfitting box'])}\")\n",
    "\n",
    "print(\"OUTSIDE DONT CARE\")\n",
    "print(f\"Overlooked pedestrians (orig. IoU = 0): {actual_overlooked_pedestrians}\")\n",
    "print(f\"Misfitting boxes (0 < orig. IoU < thresh = {iou_threshold_misfitting_box}): {actual_misfitting_annotations}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_kitti_labels(label_path):\n",
    "    boxes = []\n",
    "    with open(label_path, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            cls = parts[0]\n",
    "            # 2D bounding box: [left, top, right, bottom]\n",
    "            bbox = list(map(float, parts[4:8]))\n",
    "            boxes.append((cls, bbox))\n",
    "    return boxes\n",
    "\n",
    "def plot_and_store_label_error(image_id, pred_boxes=None, add_pred_boxes=None, scores_pred_boxes = None, scores_add_pred_boxes = None, filename=None):\n",
    "    image_id = image_id.split('.')[0]  # Remove file extension if present\n",
    "    image_path = f'{path_to_kitti}/image_2/{image_id}.png'\n",
    "    label_path = f'{path_to_kitti}label_2/{image_id}.txt'\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Draw ground truth boxes\n",
    "    gt_boxes = read_kitti_labels(label_path)\n",
    "    for cls, (left, top, right, bottom) in gt_boxes:\n",
    "        if cls == \"Pedestrian\": # we only care about pedestrians\n",
    "            pt1 = (int(left), int(top))\n",
    "            pt2 = (int(right), int(bottom))\n",
    "            cv2.rectangle(image, pt1, pt2, color=(0, 255, 0), thickness=2)\n",
    "\n",
    "    # Draw predicted boxes (if provided)\n",
    "    if pred_boxes is not None:\n",
    "        for i, pred in enumerate(pred_boxes):\n",
    "            xmin, ymin, xmax, ymax = pred[:4]\n",
    "            pt1 = (int(xmin), int(ymin))\n",
    "            pt2 = (int(xmax), int(ymax))\n",
    "            cv2.rectangle(image, pt1, pt2, color=(255, 0, 0), thickness=2)  # Red\n",
    "            if scores_pred_boxes is not None:\n",
    "                score = scores_pred_boxes[i]\n",
    "                label = f\" ({score:.4f})\"\n",
    "                cv2.putText(image, label, (int(xmin), int(ymin) - 5), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                        0.5, (255, 0, 0), 1)\n",
    "\n",
    "            # Crop the image around the center of the bounding box\n",
    "            center_x = int((xmin + xmax) / 2)\n",
    "            center_y = int((ymin + ymax) / 2)\n",
    "            crop_x_min = max(center_x - 100, 0)\n",
    "            crop_y_min = max(center_y - 100, 0)\n",
    "            crop_x_max = min(center_x + 100, image.shape[1])\n",
    "            crop_y_max = min(center_y + 100, image.shape[0])\n",
    "            cropped_image = image[crop_y_min:crop_y_max, crop_x_min:crop_x_max]\n",
    "\n",
    "            # Display the cropped image\n",
    "            plt.figure(figsize=(5, 5))\n",
    "            plt.imshow(cropped_image)\n",
    "            plt.axis('off')\n",
    "            if filename:\n",
    "                plt.savefig(filename, dpi = 100)\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# # Small but unambiguous cases. Filter for p > 0.8 and height < 40 \n",
    "# # plot and store label errors\n",
    "# np.random.seed(0) # set seed because a random subset is considered\n",
    "\n",
    "# filename_map = {}\n",
    "\n",
    "# # --- CONFIGURATION ---\n",
    "# image_dir = \"label_error_imgs/small_but_confident/\" \n",
    "# if not os.path.exists(image_dir):\n",
    "#     os.makedirs(image_dir)\n",
    "# output_file = \"label_error_imgs_small_but_confident.tex\"\n",
    "\n",
    "# label_errors = validated_gt.copy()\n",
    "# label_errors = label_errors[label_errors[\"label_error_type\"] == \"overlooked pedestrian\"]\n",
    "# label_errors = label_errors[label_errors[\"iou_with_dont_care\"] < 0.5]\n",
    "# label_errors = label_errors[label_errors[\"height\"] < 40]\n",
    "# label_errors = label_errors[label_errors[\"score\"] >= 0.8] # score is soft label probability\n",
    "# label_errors.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# N = 12\n",
    "\n",
    "# indices = np.random.choice(label_errors.index, size = N, replace = False)\n",
    "\n",
    "# for loop_index in indices:\n",
    "#     index = label_errors.index[loop_index]\n",
    "\n",
    "#     label_error = label_errors.iloc[index][['xmin', 'ymin', 'xmax', 'ymax', 'score']].values.reshape(1, -1)\n",
    "#     image_id = label_errors.iloc[index][['filename']].values[0]\n",
    "\n",
    "#     filename = f\"label_error_{loop_index}.png\"\n",
    "#     filename_map[filename] = image_id\n",
    "\n",
    "#     plot_and_store_label_error(image_id, pred_boxes=label_error, filename = image_dir + filename)\n",
    "\n",
    "\n",
    "# # write latex code for inclusion in paper\n",
    "# latex_lines = []\n",
    "# images_per_row = 4\n",
    "\n",
    "# for i, (img_name, original_name) in enumerate(filename_map.items()):\n",
    "#     if i % images_per_row == 0:\n",
    "#         latex_lines.append(r\"\\noindent\")  # Start of a new row\n",
    "\n",
    "#     latex_lines.append(\n",
    "#         rf\"\"\"\\begin{{minipage}}[t]{{0.24\\linewidth}}\n",
    "#   \\centering\n",
    "#   \\includegraphics[width=\\linewidth, , height=\\linewidth, keepaspectratio]{{imgs/{image_dir}{img_name}}}\n",
    "#   \\captionsetup{{labelformat=empty, hypcap=false}}\n",
    "#   \\captionof{{figure}}{{{original_name}}}\n",
    "# \\end{{minipage}}\"\"\"\n",
    "#     )\n",
    "\n",
    "#     if (i + 1) % images_per_row == 0:\n",
    "#         latex_lines.append(r\"\\par\")  # Space between rows with \\smallskip\n",
    "\n",
    "# # Save LaTeX code to file\n",
    "# with open(output_file, \"w\") as f:\n",
    "#     f.write(\"\\n\".join(latex_lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# # ambiguous cases. Filter for 0.5 < p < 0.8\n",
    "\n",
    "# # plot and store label errors\n",
    "# np.random.seed(0) # set seed because a random subset is considered\n",
    "\n",
    "# filename_map = {}\n",
    "\n",
    "# # --- CONFIGURATION ---\n",
    "# image_dir = \"label_error_imgs/ambiguous/\" \n",
    "# if not os.path.exists(image_dir):\n",
    "#     os.makedirs(image_dir)\n",
    "# output_file = \"label_error_imgs_ambiguous.tex\"\n",
    "\n",
    "# label_errors = validated_gt.copy()\n",
    "# label_errors = label_errors[label_errors[\"label_error_type\"] == \"overlooked pedestrian\"]\n",
    "# label_errors = label_errors[label_errors[\"iou_with_dont_care\"] < 0.5]\n",
    "# label_errors = label_errors[label_errors[\"score\"] < 0.8] # score is soft label probability filter for 0.5 is set already\n",
    "# label_errors.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# N = 12\n",
    "\n",
    "# indices = np.random.choice(label_errors.index, size = N, replace = False)\n",
    "\n",
    "# for loop_index in indices:\n",
    "#     index = label_errors.index[loop_index]\n",
    "\n",
    "#     label_error = label_errors.iloc[index][['xmin', 'ymin', 'xmax', 'ymax', 'score']].values.reshape(1, -1)\n",
    "#     image_id = label_errors.iloc[index][['filename']].values[0]\n",
    "\n",
    "#     filename = f\"label_error_{loop_index}.png\"\n",
    "#     filename_map[filename] = image_id\n",
    "\n",
    "#     plot_and_store_label_error(image_id, pred_boxes=label_error, filename = image_dir + filename)\n",
    "\n",
    "\n",
    "# # write latex code for inclusion in paper\n",
    "# latex_lines = []\n",
    "# images_per_row = 4\n",
    "\n",
    "# for i, (img_name, original_name) in enumerate(filename_map.items()):\n",
    "#     if i % images_per_row == 0:\n",
    "#         latex_lines.append(r\"\\noindent\")  # Start of a new row\n",
    "\n",
    "#     latex_lines.append(\n",
    "#         rf\"\"\"\\begin{{minipage}}[t]{{0.24\\linewidth}}\n",
    "#   \\centering\n",
    "#   \\includegraphics[width=\\linewidth, , height=\\linewidth, keepaspectratio]{{imgs/{image_dir}{img_name}}}\n",
    "#   \\captionsetup{{labelformat=empty, hypcap=false}}\n",
    "#   \\captionof{{figure}}{{{original_name}}}\n",
    "# \\end{{minipage}}\"\"\"\n",
    "#     )\n",
    "\n",
    "#     if (i + 1) % images_per_row == 0:\n",
    "#         latex_lines.append(r\"\\par\")  # Space between rows with \\smallskip\n",
    "\n",
    "# # Save LaTeX code to file\n",
    "# with open(output_file, \"w\") as f:\n",
    "#     f.write(\"\\n\".join(latex_lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# # This one requires filtering for p > 0.8 and height > 40 which was done before and needs to be added below if left out above\n",
    "# # plot and store label errors\n",
    "# np.random.seed(0) # if random subset is considered\n",
    "\n",
    "# filename_map = {}\n",
    "\n",
    "# label_errors = validated_gt.copy()\n",
    "\n",
    "# # --- CONFIGURATION ---\n",
    "# image_dir = \"label_error_imgs/top_overlooked/\" \n",
    "# if not os.path.exists(image_dir):\n",
    "#     os.makedirs(image_dir)\n",
    "# output_file = \"label_error_imgs_top_overlooked.tex\"\n",
    "\n",
    "# label_errors = label_errors[label_errors[\"label_error_type\"] == \"overlooked pedestrian\"]\n",
    "# label_errors = label_errors[label_errors[\"iou_with_dont_care\"] < 0.5]\n",
    "# label_errors = label_errors.sort_values(by=\"score\", ascending=False)\n",
    "# label_errors.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# N = len(label_errors)\n",
    "\n",
    "# for loop_index in range(N):\n",
    "#     index = label_errors.index[loop_index]\n",
    "\n",
    "#     label_error = label_errors.iloc[index][['xmin', 'ymin', 'xmax', 'ymax', 'score']].values.reshape(1, -1)\n",
    "#     image_id = label_errors.iloc[index][['filename']].values[0]\n",
    "\n",
    "#     filename = f\"label_error_{loop_index}.png\"\n",
    "#     filename_map[filename] = image_id\n",
    "\n",
    "#     plot_and_store_label_error(image_id, pred_boxes=label_error, filename = image_dir + filename)\n",
    "\n",
    "\n",
    "# # write latex code for inclusion in paper\n",
    "# latex_lines = []\n",
    "# images_per_row = 4\n",
    "\n",
    "# for i, (img_name, original_name) in enumerate(filename_map.items()):\n",
    "#     if i % images_per_row == 0:\n",
    "#         latex_lines.append(r\"\\noindent\")  # Start of a new row\n",
    "\n",
    "#     latex_lines.append(\n",
    "#         rf\"\"\"\\begin{{minipage}}[t]{{0.24\\linewidth}}\n",
    "#   \\centering\n",
    "#   \\includegraphics[width=\\linewidth, , height=\\linewidth, keepaspectratio]{{imgs/{image_dir}{img_name}}}\n",
    "#   \\captionsetup{{labelformat=empty, hypcap=false}}\n",
    "#   \\captionof{{figure}}{{{original_name}}}\n",
    "# \\end{{minipage}}\"\"\"\n",
    "#     )\n",
    "\n",
    "#     if (i + 1) % images_per_row == 0:\n",
    "#         latex_lines.append(r\"\\par\")  # Space between rows with \\smallskip\n",
    "\n",
    "# # Save LaTeX code to file\n",
    "# with open(output_file, \"w\") as f:\n",
    "#     f.write(\"\\n\".join(latex_lines))\n",
    "\n",
    "\n",
    "# ###\n",
    "\n",
    "# # label_errors = validated_gt.copy()\n",
    "# # label_errors = label_errors[label_errors[\"label_error_type\"] == \"overlooked pedestrian\"]\n",
    "# # label_errors = label_errors[label_errors[\"iou_with_dont_care\"] < 0.5]\n",
    "# # label_errors = label_errors[label_errors[\"height\"] >= 40]\n",
    "# # label_errors = label_errors[label_errors[\"score\"] >= 0.8] # score is soft label probability\n",
    "# # label_errors = label_errors.sort_values(by=\"score\", ascending=False)\n",
    "# # label_errors.reset_index(drop=True, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# # plot and store label errors\n",
    "# np.random.seed(0) # if random subset is considered\n",
    "\n",
    "# filename_map = {}\n",
    "\n",
    "# label_errors = validated_gt.copy()\n",
    "\n",
    "# # --- CONFIGURATION ---\n",
    "# image_dir = \"label_error_imgs/top_misfitting/\" \n",
    "# if not os.path.exists(image_dir):\n",
    "#     os.makedirs(image_dir)\n",
    "# output_file = \"label_error_imgs_top_misfitting.tex\"\n",
    "\n",
    "# label_errors = label_errors[label_errors[\"label_error_type\"] == \"misfitting box\"]\n",
    "# label_errors = label_errors[label_errors[\"iou_with_dont_care\"] < 0.5]\n",
    "# label_errors = label_errors.sort_values(by=\"iou_with_original_gt\", ascending=True)\n",
    "# label_errors.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# N = 20\n",
    "\n",
    "# for loop_index in range(N):\n",
    "#     index = label_errors.index[loop_index]\n",
    "\n",
    "#     label_error = label_errors.iloc[index][['xmin', 'ymin', 'xmax', 'ymax', 'score']].values.reshape(1, -1)\n",
    "#     image_id = label_errors.iloc[index][['filename']].values[0]\n",
    "\n",
    "#     filename = f\"label_error_{loop_index}.png\"\n",
    "#     filename_map[filename] = image_id\n",
    "\n",
    "#     plot_and_store_label_error(image_id, pred_boxes=label_error, filename = image_dir + filename)\n",
    "\n",
    "\n",
    "# # write latex code for inclusion in paper\n",
    "# latex_lines = []\n",
    "# images_per_row = 4\n",
    "\n",
    "# for i, (img_name, original_name) in enumerate(filename_map.items()):\n",
    "#     if i % images_per_row == 0:\n",
    "#         latex_lines.append(r\"\\noindent\")  # Start of a new row\n",
    "\n",
    "#     latex_lines.append(\n",
    "#         rf\"\"\"\\begin{{minipage}}[t]{{0.24\\linewidth}}\n",
    "#   \\centering\n",
    "#   \\includegraphics[width=\\linewidth, , height=\\linewidth, keepaspectratio]{{imgs/{image_dir}{img_name}}}\n",
    "#   \\captionsetup{{labelformat=empty, hypcap=false}}\n",
    "#   \\captionof{{figure}}{{{original_name}}}\n",
    "# \\end{{minipage}}\"\"\"\n",
    "#     )\n",
    "\n",
    "#     if (i + 1) % images_per_row == 0:\n",
    "#         latex_lines.append(r\"\\par\")  # Space between rows with \\smallskip\n",
    "\n",
    "# # Save LaTeX code to file\n",
    "# with open(output_file, \"w\") as f:\n",
    "#     f.write(\"\\n\".join(latex_lines))\n",
    "\n",
    "# # label_errors = validated_gt.copy()\n",
    "# # label_errors = label_errors[label_errors[\"label_error_type\"] == \"misfitting box\"]\n",
    "# # label_errors = label_errors[label_errors[\"iou_with_dont_care\"] < 0.5]\n",
    "# # label_errors = label_errors[label_errors[\"height\"] >= 40]\n",
    "# # label_errors = label_errors[label_errors[\"score\"] >= 0.8] # score is soft label probability\n",
    "# # label_errors = label_errors.sort_values(by=\"iou_with_original_gt\", ascending=True)\n",
    "# # label_errors.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # manually generate larger crops for three of those images.\n",
    "\n",
    "# def plot_and_store_label_error(image_id, pred_boxes=None, add_pred_boxes=None, scores_pred_boxes = None, scores_add_pred_boxes = None, filename=None):\n",
    "#     image_id = image_id.split('.')[0]  # Remove file extension if present\n",
    "#     image_path = f'{path_to_kitti}/image_2/{image_id}.png'\n",
    "#     label_path = f'{path_to_kitti}label_2/{image_id}.txt'\n",
    "#     image = cv2.imread(image_path)\n",
    "#     image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "#     # Draw ground truth boxes\n",
    "#     gt_boxes = read_kitti_labels(label_path)\n",
    "#     for cls, (left, top, right, bottom) in gt_boxes:\n",
    "#         if cls == \"Pedestrian\": # we only care about pedestrians\n",
    "#             pt1 = (int(left), int(top))\n",
    "#             pt2 = (int(right), int(bottom))\n",
    "#             cv2.rectangle(image, pt1, pt2, color=(0, 255, 0), thickness=2)\n",
    "\n",
    "#     # Draw predicted boxes (if provided)\n",
    "#     if pred_boxes is not None:\n",
    "#         for i, pred in enumerate(pred_boxes):\n",
    "#             xmin, ymin, xmax, ymax = pred[:4]\n",
    "#             pt1 = (int(xmin), int(ymin))\n",
    "#             pt2 = (int(xmax), int(ymax))\n",
    "#             cv2.rectangle(image, pt1, pt2, color=(255, 0, 0), thickness=2)  # Red\n",
    "#             if scores_pred_boxes is not None:\n",
    "#                 score = scores_pred_boxes[i]\n",
    "#                 label = f\" ({score:.4f})\"\n",
    "#                 cv2.putText(image, label, (int(xmin), int(ymin) - 5), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "#                         0.5, (255, 0, 0), 1)\n",
    "\n",
    "#             # Crop the image around the center of the bounding box\n",
    "#             center_x = int((xmin + xmax) / 2)\n",
    "#             center_y = int((ymin + ymax) / 2)\n",
    "#             crop_x_min = max(center_x - 200, 0)\n",
    "#             crop_y_min = max(center_y - 200, 0)\n",
    "#             crop_x_max = min(center_x + 200, image.shape[1])\n",
    "#             crop_y_max = min(center_y + 200, image.shape[0])\n",
    "#             cropped_image = image[crop_y_min:crop_y_max, crop_x_min:crop_x_max]\n",
    "\n",
    "#             # Display the cropped image\n",
    "#             plt.figure(figsize=(5, 5))\n",
    "#             plt.imshow(cropped_image)\n",
    "#             plt.axis('off')\n",
    "#             if filename:\n",
    "#                 plt.savefig(filename, dpi = 100)\n",
    "#             plt.show()\n",
    "\n",
    "\n",
    "# indices = [11, 16, 19]\n",
    "\n",
    "# for loop_index in indices:\n",
    "#     index = label_errors.index[loop_index]\n",
    "\n",
    "#     label_error = label_errors.iloc[index][['xmin', 'ymin', 'xmax', 'ymax', 'score']].values.reshape(1, -1)\n",
    "#     image_id = label_errors.iloc[index][['filename']].values[0]\n",
    "\n",
    "#     filename = f\"label_error_{loop_index}.png\"\n",
    "#     filename_map[filename] = image_id\n",
    "\n",
    "#     plot_and_store_label_error(image_id, pred_boxes=label_error, filename = image_dir + filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Evaluate proposed label errors of label error detection methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_label_errors(df, top_n, threshold_label_error, target_col = \"target\", prediction_col = \"prediction\"):\n",
    "    # this function returns n predictions for label errors based on the meta-regression performed by MetaDetect\n",
    "    # for this, the bounding boxes with true IoU < thresh are ordered in descending order according to the predicted IoU\n",
    "\n",
    "    df = df[ df[target_col] < threshold_label_error] # those predicted boxes that have a true IoU below the threshold are considered label error proposals\n",
    "    \n",
    "    # Note that for a metaclassification model the target col is binary and the threshold was set in Metadetect \n",
    "    df = df.sort_values(by = prediction_col, ascending=False)\n",
    "    if top_n == \"all\":\n",
    "        return df\n",
    "    else:\n",
    "        return df[:top_n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 MetaDetect\n",
    "\n",
    "The MetaDetect model predicts IoU values for all detected boxes with scores above a certain threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "## apply all steps for evaluation here for benchmarking purposes\n",
    "\n",
    "md_casc = pd.read_csv(f\"data/predictions/cascade_rcnn.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iou_threshold_TP = 0.1  # IoU threshold for considering a box as a true positive (Comparison of predicted boxes and validated GT boxes)\n",
    "\n",
    "nms_threshold = 0.5  # We can further remove overlapping boxes from the predictions via NMS. Set to None to disable NMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'target'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:3805\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3804\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3805\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3806\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'target'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[57]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m md_casc = \u001b[43mpredict_label_errors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmd_casc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mall\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miou_threshold_misfitting_box\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[48]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36mpredict_label_errors\u001b[39m\u001b[34m(df, top_n, threshold_label_error, target_col, prediction_col)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpredict_label_errors\u001b[39m(df, top_n, threshold_label_error, target_col = \u001b[33m\"\u001b[39m\u001b[33mtarget\u001b[39m\u001b[33m\"\u001b[39m, prediction_col = \u001b[33m\"\u001b[39m\u001b[33mprediction\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m      2\u001b[39m     \u001b[38;5;66;03m# this function returns n predictions for label errors based on the meta-regression performed by MetaDetect\u001b[39;00m\n\u001b[32m      3\u001b[39m     \u001b[38;5;66;03m# for this, the bounding boxes with true IoU < thresh are ordered in descending order according to the predicted IoU\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     df = df[ \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtarget_col\u001b[49m\u001b[43m]\u001b[49m < threshold_label_error] \u001b[38;5;66;03m# those predicted boxes that have a true IoU below the threshold are considered label error proposals\u001b[39;00m\n\u001b[32m      7\u001b[39m     \u001b[38;5;66;03m# Note that for a metaclassification model the target col is binary and the threshold was set in Metadetect \u001b[39;00m\n\u001b[32m      8\u001b[39m     df = df.sort_values(by = prediction_col, ascending=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.11/site-packages/pandas/core/frame.py:4102\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4100\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4101\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4102\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4103\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4104\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3807\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3808\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3809\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3810\u001b[39m     ):\n\u001b[32m   3811\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3814\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3815\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3816\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3817\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'target'"
     ]
    }
   ],
   "source": [
    "md_casc = predict_label_errors(md_casc, \"all\", iou_threshold_misfitting_box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>xmin</th>\n",
       "      <th>ymin</th>\n",
       "      <th>xmax</th>\n",
       "      <th>ymax</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000002.png</td>\n",
       "      <td>308.440491</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1242.000000</td>\n",
       "      <td>375.000000</td>\n",
       "      <td>0.060403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000002.png</td>\n",
       "      <td>6.365904</td>\n",
       "      <td>179.167618</td>\n",
       "      <td>200.370529</td>\n",
       "      <td>370.994568</td>\n",
       "      <td>0.038095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000002.png</td>\n",
       "      <td>933.774231</td>\n",
       "      <td>166.599594</td>\n",
       "      <td>972.888855</td>\n",
       "      <td>264.797424</td>\n",
       "      <td>0.026021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000002.png</td>\n",
       "      <td>688.459717</td>\n",
       "      <td>169.650024</td>\n",
       "      <td>716.337524</td>\n",
       "      <td>224.204132</td>\n",
       "      <td>0.025676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000002.png</td>\n",
       "      <td>922.277832</td>\n",
       "      <td>165.763916</td>\n",
       "      <td>1009.187500</td>\n",
       "      <td>324.516968</td>\n",
       "      <td>0.015925</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     filename        xmin        ymin         xmax        ymax     score\n",
       "0  000002.png  308.440491    0.000000  1242.000000  375.000000  0.060403\n",
       "1  000002.png    6.365904  179.167618   200.370529  370.994568  0.038095\n",
       "2  000002.png  933.774231  166.599594   972.888855  264.797424  0.026021\n",
       "3  000002.png  688.459717  169.650024   716.337524  224.204132  0.025676\n",
       "4  000002.png  922.277832  165.763916  1009.187500  324.516968  0.015925"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md_casc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1. MetaDetect on CascadeRCNN Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "object_detector_score_threshold = 0.01 # FIXED ATM. Would require re-runnning MetaDetect\n",
    "object_detector = \"cascadercnn\"\n",
    "\n",
    "iou_threshold_label_error = iou_threshold_misfitting_box  # IoU threshold for considering a predicted box as a label error proposal (Comparison of predicted boxes and original GT boxes)\n",
    "#####\n",
    "# NOTE: In MetaDetect for a metaclassification model the iou_threshold_label_error is set internally (0.5) and accordingly, Boolean values are returned -> Changing it requires re-running MetaDetect\n",
    "#####\n",
    "\n",
    "iou_threshold_TP = 0.1  # IoU threshold for considering a box as a true positive (Comparison of predicted boxes and validated GT boxes)\n",
    "\n",
    "nms_threshold = 0.5  # We can further remove overlapping boxes from the predictions via NMS. Set to None to disable NMS\n",
    "\n",
    "n = \"all\" # first n label error proposals not used anymore # n = 100\n",
    "\n",
    "from tools_label_error_detection import prepare_data, analysis_plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# md_casc = prepare_data(object_detector = object_detector, object_detector_score_threshold = object_detector_score_threshold,\n",
    "#                         filter_small_boxes = filter_small_boxes, min_bbox_height = min_bbox_height, nms_threshold = nms_threshold)\n",
    "\n",
    "md_casc = pd.read_csv(f\"data/predictions/cascade_rcnn.csv\")\n",
    "\n",
    "# X = md_casc[\"target\"].values\n",
    "# Y = md_casc[\"prediction\"].values\n",
    "# fig = plt.figure()\n",
    "# ax = plt.gca()\n",
    "# ax.scatter(X,Y, s = 3)\n",
    "# plt.xlabel(\"Target\")\n",
    "# plt.ylabel(\"Prediction\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>xmin</th>\n",
       "      <th>ymin</th>\n",
       "      <th>xmax</th>\n",
       "      <th>ymax</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000002.png</td>\n",
       "      <td>308.440491</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1242.000000</td>\n",
       "      <td>375.000000</td>\n",
       "      <td>0.060403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000002.png</td>\n",
       "      <td>6.365904</td>\n",
       "      <td>179.167618</td>\n",
       "      <td>200.370529</td>\n",
       "      <td>370.994568</td>\n",
       "      <td>0.038095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000002.png</td>\n",
       "      <td>933.774231</td>\n",
       "      <td>166.599594</td>\n",
       "      <td>972.888855</td>\n",
       "      <td>264.797424</td>\n",
       "      <td>0.026021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000002.png</td>\n",
       "      <td>688.459717</td>\n",
       "      <td>169.650024</td>\n",
       "      <td>716.337524</td>\n",
       "      <td>224.204132</td>\n",
       "      <td>0.025676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000002.png</td>\n",
       "      <td>922.277832</td>\n",
       "      <td>165.763916</td>\n",
       "      <td>1009.187500</td>\n",
       "      <td>324.516968</td>\n",
       "      <td>0.015925</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     filename        xmin        ymin         xmax        ymax     score\n",
       "0  000002.png  308.440491    0.000000  1242.000000  375.000000  0.060403\n",
       "1  000002.png    6.365904  179.167618   200.370529  370.994568  0.038095\n",
       "2  000002.png  933.774231  166.599594   972.888855  264.797424  0.026021\n",
       "3  000002.png  688.459717  169.650024   716.337524  224.204132  0.025676\n",
       "4  000002.png  922.277832  165.763916  1009.187500  324.516968  0.015925"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md_casc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # review that NMS has been applied succesfully indeed\n",
    "\n",
    "# orig_ious = []\n",
    "# for i, row in md_casc.iterrows():\n",
    "#     # Get the corresponding predicted box\n",
    "#     pred_box = torch.tensor([row['xmin'], row['ymin'], row['xmax'], row['ymax']], dtype=torch.float32).unsqueeze(0)\n",
    "    \n",
    "#     # Get the ground truth boxes for the same image\n",
    "#     other_boxes = md_casc[(md_casc['filename'] == row['filename']) & (md_casc.index != i)][['xmin', 'ymin', 'xmax', 'ymax']].values\n",
    "    \n",
    "#     if len(other_boxes) > 0:\n",
    "#         other_boxes_tensor = torch.tensor(other_boxes, dtype=torch.float32)\n",
    "#         iou = box_iou(pred_box, other_boxes_tensor).squeeze().numpy()\n",
    "#         orig_ious.append(iou.max())\n",
    "#     else:\n",
    "#         orig_ious.append(0)\n",
    "\n",
    "# np.array(orig_ious).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge with object detector predictions and export\n",
    "json_file = pd.read_json(f\"data/KITTI/predictions/results_{object_detector}_val_pedestrians_score_thresh_{object_detector_score_threshold}.json\")\n",
    "md_casc_scores = []\n",
    "for filename in json_file[\"filename\"]:\n",
    "    scores = md_casc[md_casc[\"filename\"] == filename][\"prediction\"].values\n",
    "    md_casc_scores.append(scores)\n",
    "    \n",
    "json_file[\"metadetect_scores\"] = md_casc_scores\n",
    "json_file.to_json(\"cascade_rcnn_predictions_with_exported_scores.json\", orient=\"index\", indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md_casc = predict_label_errors(md_casc, n, iou_threshold_label_error)\n",
    "md_casc.reset_index(drop=True, inplace=True)\n",
    "md_casc.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md_casc = get_ious_with_orig_and_val_gt(md_casc)\n",
    "md_casc = filter_data_for_conditions(md_casc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the first few proposed label errors\n",
    "for i in range(n_images_to_display): \n",
    "    # Display the first few proposed label errors\n",
    "    image_id = md_casc['filename'][i]\n",
    "    image_id = image_id.split('.')[0]  # Remove file extension if present\n",
    "    # Read the predicted boxes from the DataFrame\n",
    "    pred_boxes = md_casc.iloc[i][['xmin', 'ymin', 'xmax', 'ymax', 'score']].values.reshape(1, -1)\n",
    "\n",
    "    draw_boxes(f'{path_to_kitti}/image_2/{image_id}.png', f'{path_to_kitti}label_2/{image_id}.txt', pred_boxes=pred_boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # evaluate the proposed label errors\n",
    "# def evaluate_proposed_label_errors(df, iou_threshold):\n",
    "#     # this function evaluates the proposed label errors by checking if there is a box in the val. GT for which the IoU with the predicted box exceeds the threshold\n",
    "#     # df is assumed to contain the columns 'filename', 'xmin', 'ymin', 'xmax', 'ymax' and 'score' for the predicted boxes and filtered according to no IoU > thresh with original GT boxes\n",
    "#     # the column 'TP' will be added to indicate whether the predicted box is a true positive (TP) or not i.e. an actual label error\n",
    "\n",
    "\n",
    "#     df['TP'] = False  # Initialize a column for true positives\n",
    "#     for i, row in df.iterrows(): # over all proposed label errors\n",
    "#         # Get the predicted box\n",
    "#         pred_box = torch.tensor([row['xmin'], row['ymin'], row['xmax'], row['ymax']], dtype=torch.float32).unsqueeze(0)\n",
    "#         # Get the validated ground truth boxes for the same image\n",
    "#         gt_boxes = validated_gt[validated_gt['filename'] == row['filename']][['xmin', 'ymin', 'xmax', 'ymax']].values\n",
    "        \n",
    "#         if len(gt_boxes) > 0: # only if there are validated ground truth boxes for this image the prediction might be a true positive, else TP[i] simply remains False.\n",
    "#             gt_boxes_tensor = torch.tensor(gt_boxes, dtype=torch.float32)\n",
    "#             ious = box_iou(pred_box, gt_boxes_tensor).squeeze().numpy()\n",
    "#             if np.any(ious >= iou_threshold):\n",
    "#                 df.at[i, 'TP'] = True  # Mark as true positive if IoU exceeds the threshold for any of the val. GT boxes\n",
    "#     return df\n",
    "\n",
    "def evaluate_proposed_label_errors(proposal_df, validated_gt):\n",
    "\n",
    "    proposal_df[\"TP\"] = [False]*len(proposal_df)\n",
    "    overlooked_objects = 0\n",
    "    misfitting_boxes = 0\n",
    "    label_errors = 0\n",
    "    validated_gt[\"matched\"] = validated_gt[\"iou_with_original_gt\"] > iou_threshold_misfitting_box # initialize boolean column with matches of original annotations\n",
    "\n",
    "    # loop over label error proposals and match them to val GT to simulate refinement of annotations\n",
    "\n",
    "    for i, row in proposal_df.iterrows(): # assuming it is ordered according to a meta model score\n",
    "\n",
    "        proposal = torch.tensor([row['xmin'], row['ymin'], row['xmax'], row['ymax']], dtype=torch.float32).unsqueeze(0)\n",
    "            \n",
    "        # Get the val ground truth boxes for the same image\n",
    "        val_gt_boxes = validated_gt[validated_gt['filename'] == row['filename']][['xmin', 'ymin', 'xmax', 'ymax']].values\n",
    "        indices = validated_gt[validated_gt['filename'] == row['filename']][['xmin', 'ymin', 'xmax', 'ymax']].index\n",
    "        \n",
    "        if len(val_gt_boxes) > 0: # only if there are val GT annoations in this image, consider them for IoU matching, else continue with the next proposal\n",
    "            val_gt_boxes_tensor = torch.tensor(val_gt_boxes, dtype=torch.float32)\n",
    "            iou = box_iou(proposal, val_gt_boxes_tensor).squeeze().numpy()\n",
    "\n",
    "            if iou.max() > iou_threshold_TP: # if there is a matching box\n",
    "                index = indices[iou.argmax()]\n",
    "                if not validated_gt.at[index, \"matched\"]:  # Check if the box is not already matched\n",
    "                    validated_gt.at[index, \"matched\"] = True  # Flag this box as matched\n",
    "                    label_errors += 1\n",
    "                    proposal_df.at[i, \"TP\"] = True\n",
    "\n",
    "                    # here we would add IoU computation with orig. GT to distinguish overlooked and misfitting\n",
    "    return proposal_df  \n",
    "\n",
    "# label_errors =  evaluate_proposed_label_errors(md_casc, validated_gt, iou_threshold_TP)\n",
    "# print(f\"Number of proposed label errors: {len(md_casc)}\")\n",
    "# print(f\"Number of true positives: {label_errors}\", f\"TP rate: {label_errors / len(md_casc)}\")\n",
    "\n",
    "md_casc = evaluate_proposed_label_errors(md_casc, validated_gt)\n",
    "md_casc.to_json(f\"{object_detector}_proposed_label_errors_MetaDetect_kitti.json\", orient = \"index\")\n",
    "print(f\"Number of proposed label errors: {len(md_casc)}\")\n",
    "print(f\"Number of true positives: {len(md_casc[md_casc['TP'] == True])}\", f\"TP rate: {len(md_casc[md_casc['TP'] == True]) / len(md_casc)}\")\n",
    "print(f\"Number of false positives: {len(md_casc[md_casc['TP'] == False])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_plots(md_casc, score_col = \"prediction\", method=\"MetaDetect\", object_detector = \"cascadercnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md_casc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_costs_and_fnr(proposal_df, validated_gt, cost_factor, iou_threshold_TP):\n",
    "\n",
    "    # assuming proposal_df contains the label error proposals in the order of likelihood\n",
    "    # validated_gt is a dataframe containing val. GT bboxes and the IoUs of which with the original GT annotations\n",
    "    # both dataframes should be filtered according to the conditions imposed i.e. dont care regions and object height. proposal_df should also filtered for IoU < 0.5 with original annotations (only FPs)\n",
    "\n",
    "    costs = []\n",
    "    FNRs = []\n",
    "    validated_gt[\"matched\"] = validated_gt[\"iou_with_original_gt\"] > iou_threshold_TP # initialize boolean column with matches of original annotations\n",
    "\n",
    "    # initial values\n",
    "    costs.append(0)\n",
    "    FNRs.append(len(validated_gt[ validated_gt[\"matched\"] == False ]) / len(validated_gt))\n",
    "\n",
    "    # loop over label error proposals and match them to val GT to simulate refinement of annotations\n",
    "\n",
    "    for i, row in proposal_df.iterrows(): # assuming it is ordered according to a meta model score\n",
    "\n",
    "        costs.append((i+1)*cost_factor )\n",
    "\n",
    "        proposal = torch.tensor([row['xmin'], row['ymin'], row['xmax'], row['ymax']], dtype=torch.float32).unsqueeze(0)\n",
    "            \n",
    "        # Get the val ground truth boxes for the same image\n",
    "        val_gt_boxes = validated_gt[validated_gt['filename'] == row['filename']][['xmin', 'ymin', 'xmax', 'ymax']].values\n",
    "        indices = validated_gt[validated_gt['filename'] == row['filename']][['xmin', 'ymin', 'xmax', 'ymax']].index\n",
    "        \n",
    "        if len(val_gt_boxes) > 0: # only if there are val GT annoations in this image, consider them for IoU matching, else continue with the next proposal\n",
    "            val_gt_boxes_tensor = torch.tensor(val_gt_boxes, dtype=torch.float32)\n",
    "            iou = box_iou(proposal, val_gt_boxes_tensor).squeeze().numpy()\n",
    "\n",
    "            if iou.max() > iou_threshold_TP: # if there is a matching box\n",
    "                index = indices[iou.argmax()]\n",
    "                validated_gt.at[index, \"matched\"] = True  # flag this box as matched. If it was matched already, it does not count twice.\n",
    "\n",
    "        FNRs.append(len(validated_gt[ validated_gt[\"matched\"] == False ]) / len(validated_gt))\n",
    "\n",
    "    return costs, FNRs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(validated_gt[\"height\"].min(), validated_gt[\"iou_with_dont_care\"].max())\n",
    "print(original_gt[\"height\"].min(), original_gt[\"iou_with_dont_care\"].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_factor = 15.59 #  costs per bbox\n",
    "\n",
    "md_casc_costs, md_casc_FNRs = get_costs_and_fnr(md_casc, validated_gt, cost_factor, iou_threshold_TP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = plt.gca()\n",
    "ax.plot(md_casc_costs, md_casc_FNRs)\n",
    "\n",
    "plt.xlabel(\"Costs in cents\")\n",
    "plt.ylabel(\"FNR\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 MetaDetect on YOLOX Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_detector = \"yolox\"\n",
    "\n",
    "md_yolox = prepare_data(object_detector = object_detector, object_detector_score_threshold = object_detector_score_threshold,\n",
    "                        filter_small_boxes = filter_small_boxes, min_bbox_height = min_bbox_height, nms_threshold = nms_threshold)\n",
    "\n",
    "X = md_yolox[\"target\"].values\n",
    "Y = md_yolox[\"prediction\"].values\n",
    "fig = plt.figure()\n",
    "ax = plt.gca()\n",
    "ax.scatter(X,Y, s = 3)\n",
    "plt.xlabel(\"Target\")\n",
    "plt.ylabel(\"Prediction\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# review that NMS has been applied succesfully indeed\n",
    "\n",
    "orig_ious = []\n",
    "for i, row in md_yolox.iterrows():\n",
    "    # Get the corresponding predicted box\n",
    "    pred_box = torch.tensor([row['xmin'], row['ymin'], row['xmax'], row['ymax']], dtype=torch.float32).unsqueeze(0)\n",
    "    \n",
    "    # Get the ground truth boxes for the same image\n",
    "    other_boxes = md_yolox[(md_yolox['filename'] == row['filename']) & (md_yolox.index != i)][['xmin', 'ymin', 'xmax', 'ymax']].values\n",
    "    \n",
    "    if len(other_boxes) > 0:\n",
    "        other_boxes_tensor = torch.tensor(other_boxes, dtype=torch.float32)\n",
    "        iou = box_iou(pred_box, other_boxes_tensor).squeeze().numpy()\n",
    "        orig_ious.append(iou.max())\n",
    "    else:\n",
    "        orig_ious.append(0)\n",
    "\n",
    "np.array(orig_ious).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge with object detector predictions and export\n",
    "json_file = pd.read_json(f\"data/KITTI/predictions/results_{object_detector}_val_pedestrians_score_thresh_{object_detector_score_threshold}.json\")\n",
    "md_yolox_scores = []\n",
    "for filename in json_file[\"filename\"]:\n",
    "    scores = md_yolox[md_yolox[\"filename\"] == filename][\"prediction\"].values\n",
    "    md_yolox_scores.append(scores)\n",
    "    \n",
    "json_file[\"metadetect_scores\"] = md_yolox_scores\n",
    "json_file.to_json(\"yolox_predictions_with_exported_scores.json\", orient=\"index\", indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md_yolox = predict_label_errors(md_yolox, n, iou_threshold_label_error)\n",
    "md_yolox.reset_index(drop=True, inplace=True)\n",
    "md_casc = get_ious_with_orig_and_val_gt(md_casc)\n",
    "md_casc = filter_data_for_conditions(md_casc)\n",
    "md_yolox.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # compare annotations of the original ground truth with the predictions of YOLOX\n",
    "# image_ids = np.random.choice(md_yolox['filename'].unique(), size = n_images_to_display, replace=False) # Get unique image filenames\n",
    "# for image_id in image_ids: \n",
    "#     # Read the predicted boxes from the DataFrame\n",
    "#     box_indices = md_yolox[md_yolox['filename'] == image_id].index\n",
    "#     boxes = [md_yolox.iloc[i][['xmin', 'ymin', 'xmax', 'ymax', 'score']].values.reshape(1, -1) for i in box_indices]\n",
    "#     boxes = np.vstack(boxes)  # Stack all predicted boxes for this image\n",
    "#     image_id = image_id.split('.')[0]  # Remove file extension if present\n",
    "#     draw_boxes(f'{path_to_kitti}image_2/{image_id}.png', f'{path_to_kitti}label_2/{image_id}.txt', pred_boxes=boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the first few proposed label errors\n",
    "for i in range(n_images_to_display): \n",
    "    # Display the first few proposed label errors\n",
    "    image_id = md_yolox['filename'][i]\n",
    "    image_id = image_id.split('.')[0]  # Remove file extension if present\n",
    "    # Read the predicted boxes from the DataFrame\n",
    "    pred_boxes = md_yolox.iloc[i][['xmin', 'ymin', 'xmax', 'ymax', 'score']].values.reshape(1, -1)\n",
    "\n",
    "    draw_boxes(f'{path_to_kitti}/image_2/{image_id}.png', f'{path_to_kitti}label_2/{image_id}.txt', pred_boxes=pred_boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md_yolox = evaluate_proposed_label_errors(md_yolox, validated_gt)\n",
    "md_yolox.to_json(f\"{object_detector}_proposed_label_errors_MetaDetect_kitti.json\", orient = \"index\")\n",
    "print(f\"Number of proposed label errors: {len(md_yolox)}\")\n",
    "print(f\"Number of true positives: {len(md_yolox[md_yolox['TP'] == True])}\")\n",
    "print(f\"Number of false positives: {len(md_yolox[md_yolox['TP'] == False])}\")\n",
    "print(f\"TP rate: {len(md_yolox[md_yolox['TP'] == True]) / len(md_yolox)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_plots(md_yolox, score_col = \"prediction\", method=\"MetaDetect\", object_detector = \"yolox\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md_yolox = get_ious_with_orig_and_val_gt(md_yolox)\n",
    "md_yolox = filter_data_for_conditions(md_yolox)\n",
    "\n",
    "md_yolox_costs, md_yolox_FNRs = get_costs_and_fnr(md_yolox, validated_gt, cost_factor, iou_threshold_TP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = plt.gca()\n",
    "ax.plot(md_casc_costs, md_casc_FNRs, label = \"Cascade R-CNN + MetaDetect\")\n",
    "ax.plot(md_yolox_costs, md_yolox_FNRs, label = \"YOLOX + MetaDetect\")\n",
    "\n",
    "plt.xlabel(\"Costs in cents\")\n",
    "plt.ylabel(\"FNR\")\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig(\"plots/FNRS.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Cleanlab\n",
    "\n",
    "Needs to be updated such that determining label errors is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cleanlab.object_detection.filter import find_label_issues, compute_scores_boxwise\n",
    "from cleanlab.object_detection.rank import (\n",
    "    _separate_label,\n",
    "    _separate_prediction,\n",
    "    get_label_quality_scores,\n",
    "    issues_from_scores,\n",
    ")\n",
    "from cleanlab.object_detection.summary import visualize\n",
    "\n",
    "from tools_label_error_detection import construct_cleanlab_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 Cleanlab on Cascade RCNN Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 8\n",
    "thresh = 0.01\n",
    "object_detector = \"cascadercnn\"\n",
    "\n",
    "path_to_predictions = f\"data/KITTI/predictions/results_{object_detector}_val_pedestrians_score_thresh_{0.01}.json\"\n",
    "\n",
    "labels, predictions = construct_cleanlab_input(path_to_kitti, split, path_to_predictions, n_classes, thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get boolean vector of label issues for all images\n",
    "has_label_issue = find_label_issues(labels, predictions, return_indices_ranked_by_score=True)\n",
    "num_examples_to_show = n_images_to_display # view this many images flagged with the most severe label issues\n",
    "has_label_issue[:num_examples_to_show]\n",
    "\n",
    "# to get label quality scores for all images\n",
    "label_quality_scores = get_label_quality_scores(labels, predictions)\n",
    "label_quality_scores[:num_examples_to_show]\n",
    "\n",
    "issue_idx = issues_from_scores(label_quality_scores, threshold=0.5)  # lower threshold will return fewer (but more confident) label issues\n",
    "issue_idx[:num_examples_to_show], label_quality_scores[issue_idx][:num_examples_to_show]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in issue_idx[:num_examples_to_show]:\n",
    "    issue_to_visualize = idx  # change this to view other images\n",
    "    class_names = {\"2\": \"pedestrian\"}\n",
    "\n",
    "    label = labels[issue_to_visualize]\n",
    "    prediction = predictions[issue_to_visualize]\n",
    "    score = label_quality_scores[issue_to_visualize]\n",
    "    image_path = os.path.join(path_to_kitti, 'image_2', label['filename'])  # Assuming images are in 'image_2' folder\n",
    "    # image_path = \"/home/penquitt/KITTI_pedestrian/val/images/\" + label['img_name']\n",
    "\n",
    "    print(image_path, '| idx', issue_to_visualize , '| label quality score:', score, '| is issue: True')\n",
    "    visualize(image_path, label=label, prediction=prediction, class_names=class_names, overlay=False, save_path=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bad loc scores and swapped scores are computed for every bbox in the GT\n",
    "# overlooked_scores are computed for every bbox in the predictions\n",
    "\n",
    "# internal high probability threshold is 0.95 which is way to high for our purposes -> changed it to 0 so that all predictions are considered\n",
    "\n",
    "# Now it only computes scores if the predicted prob. is above the threshold AND there is no GT bbox with a non-zero IoU\n",
    "# One might consider introducing a lower threshold for the IoU by means of which a bbox is considered as being \"overlooked\"?\n",
    "\n",
    "overlooked_scores, bad_loc_scores, swapped_scores = compute_scores_boxwise(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill nan values with 1 and then set 1 - scores as the probability for the object to be overlooked\n",
    "overlooked_scores = [np.nan_to_num(arr, nan=1.0) for arr in overlooked_scores]\n",
    "overlooked_probs = [1 - arr for arr in overlooked_scores]\n",
    "overlooked_probs = np.concatenate([arr for arr in overlooked_probs if arr.size > 0])\n",
    "\n",
    "bad_loc_scores = [np.nan_to_num(arr, nan=1.0) for arr in bad_loc_scores]\n",
    "bad_loc_probs = [1 - arr for arr in bad_loc_scores]\n",
    "bad_loc_probs = np.concatenate([arr for arr in bad_loc_probs if arr.size > 0])\n",
    "\n",
    "swapped_scores = [np.nan_to_num(arr, nan=1.0) for arr in swapped_scores]\n",
    "swapped_probs = [1 - arr for arr in swapped_scores]\n",
    "swapped_probs = np.concatenate([arr for arr in swapped_probs if arr.size > 0])\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(15, 4))\n",
    "ax[0].hist(overlooked_probs, bins=50, alpha=0.7)\n",
    "ax[0].set_title('Overlooked')\n",
    "ax[0].set_xlabel('Probability')\n",
    "ax[0].set_ylabel('Frequency')\n",
    "ax[1].hist(bad_loc_probs, bins=50, alpha=0.7)\n",
    "ax[1].set_title('Bad Location')\n",
    "ax[1].set_xlabel('Probability')\n",
    "ax[1].set_ylabel('Frequency')\n",
    "ax[2].hist(swapped_probs, bins=50, alpha=0.7)\n",
    "ax[2].set_title('Swapped Class')\n",
    "ax[2].set_xlabel('Probability')\n",
    "ax[2].set_ylabel('Frequency')\n",
    "\n",
    "plt.savefig(f\"plots/cleanlab_scores_{object_detector}.png\", bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bad location score -> label error proposals\n",
    "bad_loc_df = pd.DataFrame({\n",
    "    'filename': [label['filename'] for label in labels for _ in range(len(label['bboxes']))],\n",
    "    'xmin': [bbox[0] for label in labels for bbox in label['bboxes']],\n",
    "    'ymin': [bbox[1] for label in labels for bbox in label['bboxes']],\n",
    "    'xmax': [bbox[2] for label in labels for bbox in label['bboxes']],\n",
    "    'ymax': [bbox[3] for label in labels for bbox in label['bboxes']],\n",
    "    'score': [score for label_scores in bad_loc_scores for score in label_scores],\n",
    "    'prob': [1 - score for label_scores in bad_loc_scores for score in label_scores],\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get IoU with validated GT for each GT box in the bad location DataFrame to determine label errors of this type\n",
    "bad_loc_df = get_ious_with_orig_and_val_gt(bad_loc_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_loc_df = bad_loc_df.sort_values(by = \"prob\", ascending=False)\n",
    "\n",
    "bad_loc_df[\"TP\"] = bad_loc_df[\"iou_with_val_gt\"] < iou_threshold_misfitting_box\n",
    "\n",
    "print(f\"Number of GT Boxes: {len(bad_loc_df)}\")\n",
    "print(f\"Out of which label errors due to bad location: {len(bad_loc_df[bad_loc_df['TP'] == True])}\")\n",
    "\n",
    "print(bad_loc_df[\"iou_with_val_gt\"].min()) # 0 -> there are boxes in the original GT that are not in the val. GT\n",
    "\n",
    "bad_loc_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = bad_loc_df[\"prob\"].values\n",
    "Y = bad_loc_df[\"iou_with_val_gt\"].values\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.gca()\n",
    "ax.scatter(X,Y, s = 3)\n",
    "plt.xlabel(\"Pred. Probability of Bad Location\")\n",
    "plt.ylabel(\"IoU of GT box with Val. GT\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_plots(bad_loc_df, score_col = \"prob\", method=\"Cleanlab_bad_loc\", object_detector = \"cascadercnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overlooked score -> prediction error proposals\n",
    "overlooked_df = pd.DataFrame({\n",
    "    'filename': [labels[i]['filename'] for i in range(len(predictions)) for _ in range(len(predictions[i][2]))], # labels and predictions are per image in the same order of images\n",
    "    'xmin': [bbox[0] for prediction in predictions for bbox in prediction[2]],\n",
    "    'ymin': [bbox[1] for prediction in predictions for bbox in prediction[2]],\n",
    "    'xmax': [bbox[2] for prediction in predictions for bbox in prediction[2]],\n",
    "    'ymax': [bbox[3] for prediction in predictions for bbox in prediction[2]],\n",
    "    'cleanlab_overlooked_score': [score for prediction_scores in overlooked_scores for score in prediction_scores],\n",
    "    'prob': [float(1.0 - score) for prediction_scores in overlooked_scores for score in prediction_scores],\n",
    "    })\n",
    "\n",
    "# # get IoU with original GT for each predicted box to select label error proposals (IoU < 0.5)\n",
    "# # get IoU with validated GT for each predicted box in the overlooked DataFrame to determine label errors of this type\n",
    "overlooked_df = get_ious_with_orig_and_val_gt(overlooked_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge with object detector predictions and export\n",
    "json_file = pd.read_json(\"cascade_rcnn_predictions_with_exported_scores.json\").T # transpose here\n",
    "cleanlab_scores = []\n",
    "for filename in json_file[\"filename\"]:\n",
    "    scores = overlooked_df[overlooked_df[\"filename\"] == filename][\"prob\"].values\n",
    "    cleanlab_scores.append(scores)\n",
    "    \n",
    "json_file[\"cleanlab_overlooked_scores\"] = cleanlab_scores\n",
    "json_file.to_json(\"cascade_rcnn_predictions_with_exported_scores.json\", orient=\"index\", indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get label error proposals and evaluate them\n",
    "overlooked_df = overlooked_df[overlooked_df['iou_with_original_gt'] < iou_threshold_label_error]  # Filter for label error proposals (orig. IoU < threshold)\n",
    "overlooked_df = overlooked_df.sort_values(by = \"prob\", ascending=False)\n",
    "\n",
    "overlooked_df[\"TP\"] = overlooked_df[\"iou_with_val_gt\"] >= iou_threshold_TP\n",
    "\n",
    "print(f\"Number of predicted Boxes: {len(overlooked_df)}\")\n",
    "print(f\"Number of Label Errors detected through predicted boxes: {len(overlooked_df[(overlooked_df['TP'] == True)])}\")\n",
    "print(f\"Out of which label errors due to the box being overlooked previously (orig. IoU = 0): {len(overlooked_df[(overlooked_df['TP'] == True) & (overlooked_df['iou_with_original_gt'] == 0)])}\")\n",
    "print(f\"Out of which label errors due to a misfitting bbox (0 < orig. IoU < thresh = {iou_threshold_label_error}): {len(overlooked_df[(overlooked_df['TP'] == True) & (overlooked_df['iou_with_original_gt'] > 0)])}\")\n",
    "overlooked_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = overlooked_df[\"prob\"].values\n",
    "Y = overlooked_df[\"iou_with_val_gt\"].values\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.gca()\n",
    "ax.scatter(X,Y, s = 3)\n",
    "plt.xlabel(\"Pred. Probability of Overlooked Object\")\n",
    "plt.ylabel(\"IoU of GT box with Val. GT\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_plots(overlooked_df, score_col = \"prob\", method=\"Cleanlab_overlooked\", object_detector = \"cascadercnn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 Cleanlab on YOLOX Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_detector = \"yolox\"\n",
    "\n",
    "path_to_predictions = f\"data/KITTI/predictions/results_{object_detector}_val_pedestrians_score_thresh_{0.01}.json\"\n",
    "\n",
    "labels, predictions = construct_cleanlab_input(path_to_kitti, split, path_to_predictions, n_classes, thresh)\n",
    "\n",
    "# to get boolean vector of label issues for all images\n",
    "has_label_issue = find_label_issues(labels, predictions, return_indices_ranked_by_score=True)\n",
    "num_examples_to_show = n_images_to_display # view this many images flagged with the most severe label issues\n",
    "has_label_issue[:num_examples_to_show]\n",
    "\n",
    "# to get label quality scores for all images\n",
    "label_quality_scores = get_label_quality_scores(labels, predictions)\n",
    "label_quality_scores[:num_examples_to_show]\n",
    "\n",
    "issue_idx = issues_from_scores(label_quality_scores, threshold=0.5)  # lower threshold will return fewer (but more confident) label issues\n",
    "issue_idx[:num_examples_to_show], label_quality_scores[issue_idx][:num_examples_to_show]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in issue_idx[:num_examples_to_show]:\n",
    "    issue_to_visualize = idx  # change this to view other images\n",
    "    class_names = {\"2\": \"pedestrian\"}\n",
    "\n",
    "    label = labels[issue_to_visualize]\n",
    "    prediction = predictions[issue_to_visualize]\n",
    "    score = label_quality_scores[issue_to_visualize]\n",
    "    image_path = os.path.join(path_to_kitti, 'image_2', label['filename'])  # Assuming images are in 'image_2' folder\n",
    "    # image_path = \"/home/penquitt/KITTI_pedestrian/val/images/\" + label['img_name']\n",
    "\n",
    "    print(image_path, '| idx', issue_to_visualize , '| label quality score:', score, '| is issue: True')\n",
    "    visualize(image_path, label=label, prediction=prediction, class_names=class_names, overlay=False, save_path=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlooked_scores, bad_loc_scores, swapped_scores = compute_scores_boxwise(labels, predictions)\n",
    "\n",
    "# fill nan values with 1 and then set 1 - scores as the probability for the object to be overlooked\n",
    "overlooked_scores = [np.nan_to_num(arr, nan=1.0) for arr in overlooked_scores]\n",
    "overlooked_probs = [1 - arr for arr in overlooked_scores]\n",
    "overlooked_probs = np.concatenate([arr for arr in overlooked_probs if arr.size > 0])\n",
    "\n",
    "bad_loc_scores = [np.nan_to_num(arr, nan=1.0) for arr in bad_loc_scores]\n",
    "bad_loc_probs = [1 - arr for arr in bad_loc_scores]\n",
    "bad_loc_probs = np.concatenate([arr for arr in bad_loc_probs if arr.size > 0])\n",
    "\n",
    "swapped_scores = [np.nan_to_num(arr, nan=1.0) for arr in swapped_scores]\n",
    "swapped_probs = [1 - arr for arr in swapped_scores]\n",
    "swapped_probs = np.concatenate([arr for arr in swapped_probs if arr.size > 0])\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(15, 4))\n",
    "ax[0].hist(overlooked_probs, bins=50, alpha=0.7)\n",
    "ax[0].set_title('Overlooked')\n",
    "ax[0].set_xlabel('Probability')\n",
    "ax[0].set_ylabel('Frequency')\n",
    "ax[1].hist(bad_loc_probs, bins=50, alpha=0.7)\n",
    "ax[1].set_title('Bad Location')\n",
    "ax[1].set_xlabel('Probability')\n",
    "ax[1].set_ylabel('Frequency')\n",
    "ax[2].hist(swapped_probs, bins=50, alpha=0.7)\n",
    "ax[2].set_title('Swapped Class')\n",
    "ax[2].set_xlabel('Probability')\n",
    "ax[2].set_ylabel('Frequency')\n",
    "\n",
    "plt.savefig(f\"plots/cleanlab_scores_{object_detector}.png\", bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bad location score -> label error proposals\n",
    "bad_loc_df = pd.DataFrame({\n",
    "    'filename': [label['filename'] for label in labels for _ in range(len(label['bboxes']))],\n",
    "    'xmin': [bbox[0] for label in labels for bbox in label['bboxes']],\n",
    "    'ymin': [bbox[1] for label in labels for bbox in label['bboxes']],\n",
    "    'xmax': [bbox[2] for label in labels for bbox in label['bboxes']],\n",
    "    'ymax': [bbox[3] for label in labels for bbox in label['bboxes']],\n",
    "    'score': [score for label_scores in bad_loc_scores for score in label_scores],\n",
    "    'prob': [1 - score for label_scores in bad_loc_scores for score in label_scores],\n",
    "    })\n",
    "\n",
    "# get IoU with validated GT for each GT box in the bad location DataFrame to determine label errors of this type\n",
    "bad_loc_df = get_ious_with_orig_and_val_gt(bad_loc_df)\n",
    "\n",
    "bad_loc_df = bad_loc_df.sort_values(by = \"prob\", ascending=False)\n",
    "\n",
    "bad_loc_df[\"TP\"] = bad_loc_df[\"iou_with_val_gt\"] < iou_threshold_misfitting_box\n",
    "\n",
    "print(f\"Number of GT Boxes: {len(bad_loc_df)}\")\n",
    "print(f\"Out of which label errors due to bad location: {len(bad_loc_df[bad_loc_df['TP'] == True])}\")\n",
    "\n",
    "X = bad_loc_df[\"prob\"].values\n",
    "Y = bad_loc_df[\"iou_with_val_gt\"].values\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.gca()\n",
    "ax.scatter(X,Y, s = 3)\n",
    "plt.xlabel(\"Pred. Probability of Bad Location\")\n",
    "plt.ylabel(\"IoU of GT box with Val. GT\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_plots(bad_loc_df, score_col = \"prob\", method=\"Cleanlab_bad_loc\", object_detector = \"cascadercnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overlooked score -> prediction error proposals\n",
    "overlooked_df = pd.DataFrame({\n",
    "    'filename': [labels[i]['filename'] for i in range(len(predictions)) for _ in range(len(predictions[i][2]))], # labels and predictions are per image in the same order of images\n",
    "    'xmin': [bbox[0] for prediction in predictions for bbox in prediction[2]],\n",
    "    'ymin': [bbox[1] for prediction in predictions for bbox in prediction[2]],\n",
    "    'xmax': [bbox[2] for prediction in predictions for bbox in prediction[2]],\n",
    "    'ymax': [bbox[3] for prediction in predictions for bbox in prediction[2]],\n",
    "    'cleanlab_overlooked_score': [score for prediction_scores in overlooked_scores for score in prediction_scores],\n",
    "    'prob': [float(1.0 - score) for prediction_scores in overlooked_scores for score in prediction_scores],\n",
    "    })\n",
    "\n",
    "# # get IoU with original GT for each predicted box to select label error proposals (IoU < 0.5)\n",
    "# # get IoU with validated GT for each predicted box in the overlooked DataFrame to determine label errors of this type\n",
    "overlooked_df = get_ious_with_orig_and_val_gt(overlooked_df)\n",
    "\n",
    "# merge with object detector predictions and export\n",
    "json_file = pd.read_json(\"yolox_predictions_with_exported_scores.json\").T # transpose here\n",
    "cleanlab_scores = []\n",
    "for filename in json_file[\"filename\"]:\n",
    "    scores = overlooked_df[overlooked_df[\"filename\"] == filename][\"prob\"].values\n",
    "    cleanlab_scores.append(scores)\n",
    "    \n",
    "json_file[\"cleanlab_overlooked_scores\"] = cleanlab_scores\n",
    "json_file.to_json(\"yolox_predictions_with_exported_scores.json\", orient=\"index\", indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Loss-based Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_detector = \"cascadercnn\"\n",
    "\n",
    "loss_based = pd.read_csv(f\"data/KITTI/predictions/results_loss_based_method.csv\").drop(columns=[\"Unnamed: 0\"])\n",
    "loss_based[\"filename\"] = [path.split(\"/\")[-1] for path in loss_based[\"img_path\"]]  # get the image name from the path\n",
    "loss_based = loss_based[loss_based['class_id'] == 2]  # Filter for pedestrian predictions \n",
    "loss_based = loss_based.rename(columns={\"s\": \"score\"})\n",
    "loss_based = loss_based[loss_based[\"score\"] > object_detector_score_threshold]\n",
    "loss_based[\"height\"] = loss_based[\"ymax\"] - loss_based[\"ymin\"]\n",
    "if filter_small_boxes:\n",
    "    print(f\"Filtering out {len(loss_based[loss_based['height'] < min_bbox_height])} boxes with height < {min_bbox_height} pixels\")\n",
    "    loss_based = loss_based[loss_based[\"height\"] >= min_bbox_height]  # Filter out boxes that are too small\n",
    "    loss_based.reset_index(drop=True, inplace=True)  # Reset the index\n",
    "\n",
    "# proposed label errors do not overlap much due to NMS being applied in the object detector already.\n",
    "#  Here, we could further reduce this threshold if we wanted to.\n",
    "# nms_df = perform_nms_on_dataframe(df, nms_threshold)\n",
    "# print(f\"Number of Boxes: {len(df)}\", f\", After NMS: {len(nms_df)}\" )\n",
    "\n",
    "loss_based = get_ious_with_orig_and_val_gt(loss_based)\n",
    "print(len(loss_based))\n",
    "loss_based = predict_label_errors(loss_based, n, iou_threshold_label_error, target_col = \"iou_with_original_gt\", prediction_col = \"rpn_s\")\n",
    "loss_based.reset_index(drop=True, inplace=True)\n",
    "print(len(loss_based))\n",
    "loss_based.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_based[\"ymax\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md_casc[\"ymax\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = loss_based[\"iou_with_original_gt\"].values\n",
    "Y = loss_based[\"iou_with_val_gt\"].values\n",
    "fig = plt.figure()\n",
    "ax = plt.gca()\n",
    "ax.scatter(X,Y, s = 3)\n",
    "plt.xlabel(\"IoU original GT\")\n",
    "plt.ylabel(\"IoU val. GT\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "image_id = loss_based['filename'][i]\n",
    "image_id = image_id.split('.')[0]  # Remove file extension if present\n",
    "# Read the predicted box from the DataFrame\n",
    "\n",
    "# pred_boxes = loss_based.iloc[i][['xmin', 'ymin', 'xmax', 'ymax', 'score']].values.reshape(1, -1)\n",
    "pred_boxes = [np.array([348, 358, 373, 460, 0.33])]\n",
    "\n",
    "def read_kitti_labels(label_path):\n",
    "    boxes = []\n",
    "    with open(label_path, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            cls = parts[0]\n",
    "            # 2D bounding box: [left, top, right, bottom]\n",
    "            bbox = list(map(float, parts[4:8]))\n",
    "            boxes.append((cls, bbox))\n",
    "    return boxes\n",
    "\n",
    "\n",
    "image_path = f'{path_to_kitti}/image_2/{image_id}.png'\n",
    "label_path = f'{path_to_kitti}label_2/{image_id}.txt'\n",
    "scores_pred_boxes = None\n",
    "image = cv2.imread(image_path)\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Draw ground truth boxes\n",
    "gt_boxes = read_kitti_labels(label_path)\n",
    "for cls, (left, top, right, bottom) in gt_boxes:\n",
    "    if cls == \"Pedestrian\": # we only care about pedestrians\n",
    "        pt1 = (int(left), int(top))\n",
    "        pt2 = (int(right), int(bottom))\n",
    "        print(left, top, right, bottom)\n",
    "        cv2.rectangle(image, pt1, pt2, color=(0, 255, 0), thickness=2)  # Green\n",
    "        # cv2.putText(image, \"GT\", (int(left), int(top) - 5), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "        #     0.5, (0, 255, 0), 1)\n",
    "\n",
    "# Draw predicted boxes (if provided)\n",
    "if pred_boxes is not None:\n",
    "    for i, pred in enumerate(pred_boxes):\n",
    "        xmin, ymin, xmax, ymax = pred[:4]\n",
    "\n",
    "        print(xmin, ymin, xmax, ymax)\n",
    "        pt1 = (int(xmin), int(ymin/2))\n",
    "        pt2 = (int(xmax), int(ymax/2))\n",
    "        cv2.rectangle(image, pt1, pt2, color=(255, 0, 0), thickness=2)  # Red\n",
    "        if scores_pred_boxes is not None:\n",
    "            score = scores_pred_boxes[i]\n",
    "            label = f\" ({score:.4f})\"\n",
    "            cv2.putText(image, label, (int(xmin), int(ymin) - 5), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                        0.5, (255, 0, 0), 1)\n",
    "            \n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.imshow(image)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_images_to_display = 5\n",
    "# display the first few proposed label errors\n",
    "for i in range(n_images_to_display): \n",
    "    # Display the first few proposed label errors\n",
    "    image_id = loss_based['filename'][i]\n",
    "    image_id = image_id.split('.')[0]  # Remove file extension if present\n",
    "    # Read the predicted box from the DataFrame\n",
    "    pred_boxes = loss_based.iloc[i][['xmin', 'ymin', 'xmax', 'ymax', 'score']].values.reshape(1, -1)\n",
    "\n",
    "    draw_boxes(f'{path_to_kitti}/image_2/{image_id}.png', f'{path_to_kitti}label_2/{image_id}.txt', pred_boxes=pred_boxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Analyis of bbox sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the validated ground truth data\n",
    "validated_gt = pd.read_csv(path_to_validated_gt + f\"validated_gt_{validated_gt_prob_threshold}.csv\")\n",
    "# Ensure the 'filename' column is in the correct format\n",
    "validated_gt[\"filename\"] = validated_gt['filename'].astype(str).str.zfill(6) + '.png'\n",
    "# Convert box format from x_mean, y_mean, width, height to xmin, ymin, xmax, ymax\n",
    "validated_gt[\"xmin\"] = validated_gt[\"bbox\"].apply(lambda x: ast.literal_eval(x)[0] - ast.literal_eval(x)[2] / 2)\n",
    "validated_gt[\"ymin\"] = validated_gt[\"bbox\"].apply(lambda x: ast.literal_eval(x)[1] - ast.literal_eval(x)[3] / 2)\n",
    "validated_gt[\"xmax\"] = validated_gt[\"bbox\"].apply(lambda x: ast.literal_eval(x)[0] + ast.literal_eval(x)[2] / 2)\n",
    "validated_gt[\"ymax\"] = validated_gt[\"bbox\"].apply(lambda x: ast.literal_eval(x)[1] + ast.literal_eval(x)[3] / 2)\n",
    "\n",
    "validated_gt[\"height\"] = validated_gt[\"ymax\"] - validated_gt[\"ymin\"]\n",
    "\n",
    "original_gt = construct_original_gt(path_to_kitti, split, False, 0) # No filtering applied\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_fontsize = 9\n",
    "very_small_fontsize = 6\n",
    "\n",
    "# compare histograms of the bbox sizes of the original and validated ground truth\n",
    "original_gt['area'] = (original_gt['xmax'] - original_gt['xmin']) * (original_gt['ymax'] - original_gt['ymin'])\n",
    "validated_gt['area'] = (validated_gt['xmax'] - validated_gt['xmin']) * (validated_gt['ymax'] - validated_gt['ymin'])\n",
    "\n",
    "fig = plt.figure(figsize=(1.7,1.25))\n",
    "ax = plt.gca()\n",
    "ax.hist(original_gt['area'], bins=np.logspace(np.log10(original_gt['area'].min()), np.log10(original_gt['area'].max()), 50), alpha=0.5, label='Original GT')\n",
    "ax.hist(validated_gt['area'], bins=np.logspace(np.log10(validated_gt['area'].min()), np.log10(validated_gt['area'].max()), 50), alpha=0.5, label='Validated GT')\n",
    "plt.xlabel('Bounding Box Area', fontsize = small_fontsize)\n",
    "plt.ylabel('Frequency', fontsize = small_fontsize)\n",
    "\n",
    "plt.xscale('log')  # Log scale for better visibility of small areas\n",
    "plt.grid()\n",
    "\n",
    "plt.yticks([0,20,40, 60], fontsize = small_fontsize)\n",
    "plt.xticks([10, 1000, 100000], fontsize = small_fontsize)\n",
    "\n",
    "plt.legend(fontsize=very_small_fontsize, ncols=2, bbox_to_anchor=(0.34, 1.1), loc='center', columnspacing = 0.75)\n",
    "\n",
    "plt.savefig(f\"plots/bbox_area_histogram_kitti_orig_and_val_gt.png\", bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md_casc = prepare_data(object_detector = \"cascadercnn\", object_detector_score_threshold = object_detector_score_threshold,\n",
    "                        filter_small_boxes = False, min_bbox_height = 0, nms_threshold = 0.5)\n",
    "\n",
    "md_yolox = prepare_data(object_detector = \"yolox\", object_detector_score_threshold = object_detector_score_threshold,\n",
    "                        filter_small_boxes = False, min_bbox_height = 0, nms_threshold = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md_casc[\"area\"] = (md_casc['xmax'] - md_casc['xmin']) * (md_casc['ymax'] - md_casc['ymin'])\n",
    "\n",
    "md_yolox[\"area\"] = (md_yolox['xmax'] - md_yolox['xmin']) * (md_yolox['ymax'] - md_yolox['ymin'])\n",
    "\n",
    "alpha = 0.7\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.gca()\n",
    "ax.hist(md_casc['area'], bins=np.logspace(np.log10(original_gt['area'].min()), np.log10(original_gt['area'].max()), 50), alpha=alpha, label='Predictions Cascade R-CNN')\n",
    "ax.hist(md_yolox['area'], bins=np.logspace(np.log10(original_gt['area'].min()), np.log10(original_gt['area'].max()), 50), alpha=alpha, label='Predictions YOLOX')\n",
    "ax.hist(validated_gt['area'], bins=np.logspace(np.log10(validated_gt['area'].min()), np.log10(validated_gt['area'].max()), 50), alpha=alpha, label='Validated GT')\n",
    "\n",
    "\n",
    "plt.xlabel('Bounding Box Area')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "#plt.title('Histogram of Bounding Box Areas in Original and Validated GT')\n",
    "plt.xscale('log')  # Log scale for better visibility of small areas\n",
    "plt.grid()\n",
    "\n",
    "plt.savefig(f\"plots/bbox_area_histogram_object_detectors.png\", bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Distribution of ambiguity and correlation with object size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the validated ground truth data\n",
    "val_gt_zero = pd.read_csv(path_to_validated_gt + f\"validated_gt_{0.0}.csv\")\n",
    "# Ensure the 'filename' column is in the correct format\n",
    "val_gt_zero[\"filename\"] = val_gt_zero['filename'].astype(str).str.zfill(6) + '.png'\n",
    "# Convert box format from x_mean, y_mean, width, height to xmin, ymin, xmax, ymax\n",
    "val_gt_zero[\"xmin\"] = val_gt_zero[\"bbox\"].apply(lambda x: ast.literal_eval(x)[0] - ast.literal_eval(x)[2] / 2)\n",
    "val_gt_zero[\"ymin\"] = val_gt_zero[\"bbox\"].apply(lambda x: ast.literal_eval(x)[1] - ast.literal_eval(x)[3] / 2)\n",
    "val_gt_zero[\"xmax\"] = val_gt_zero[\"bbox\"].apply(lambda x: ast.literal_eval(x)[0] + ast.literal_eval(x)[2] / 2)\n",
    "val_gt_zero[\"ymax\"] = val_gt_zero[\"bbox\"].apply(lambda x: ast.literal_eval(x)[1] + ast.literal_eval(x)[3] / 2)\n",
    "\n",
    "val_gt_zero[\"area\"] = (val_gt_zero['xmax'] - val_gt_zero['xmin']) * (val_gt_zero['ymax'] - val_gt_zero['ymin'])\n",
    "\n",
    "# plot distribution of probability for the bounding box containing a pedestrian\n",
    "\n",
    "fig = plt.figure(figsize=(1.7,1.25))\n",
    "ax = plt.gca()\n",
    "ax.hist(val_gt_zero['score'], 25, ec = \"darkslategrey\")\n",
    "plt.xlabel('Probability', fontsize = small_fontsize)\n",
    "plt.ylabel('Frequency', fontsize = small_fontsize)\n",
    "plt.yticks([0,100,200,300], fontsize = small_fontsize)\n",
    "plt.xticks([0,0.5, 1], fontsize = small_fontsize)\n",
    "plt.grid(zorder=0)\n",
    "ax.set_axisbelow(True)\n",
    "plt.savefig(f\"plots/soft_label_distribution.png\", bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatter plot of probability for the bounding box containing a pedestrian and bbox area\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.gca()\n",
    "ax.scatter(val_gt_zero['score'], val_gt_zero['area'], s = 3)\n",
    "plt.ylabel('Bounding Box Area')\n",
    "plt.xlabel('Probability')\n",
    "#plt.yticks([0,100,200,300])\n",
    "plt.grid(zorder=0)\n",
    "ax.set_axisbelow(True)\n",
    "\n",
    "plt.yscale(\"log\")\n",
    "\n",
    "# plt.savefig(f\"plots/correlation_ambiguity_object_size.png\", bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Analysis of Detected Label Errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Cascade R-CNN Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Interesting is the difference between the IoU with the original GT and the IoU with the validated GT as this kind of indicates to which extent the box was misfitting or not annotated at all\n",
    "# # Small difference in IoU indicates that the box was misfitting, large difference indicates that the box was not annotated at all in the original GT\n",
    "# # Very small difference in IoU may indicate non-severe label errors\n",
    "\n",
    "md_casc = get_ious_with_orig_and_val_gt(md_casc)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.gca()\n",
    "ax.scatter(md_casc.query(\"TP == True\")['iou_with_original_gt'], md_casc.query(\"TP == True\")['iou_with_val_gt'], s = 4)\n",
    "\n",
    "plt.xlabel('IoU with original GT')\n",
    "plt.ylabel('IoU with validated GT')\n",
    "plt.grid(zorder=0)\n",
    "ax.set_axisbelow(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
